[{"authors":null,"categories":null,"content":"Charlie Lehman is a Machine Learning Ph.D. student at the Georgia Tech Omni Lab fro Visual Engineering and Science (OLIVES). His research interests include robustness and explainability of deep vision models. He is also an Engineering Duty Officer with the U.S. Navy Reserves, where he specializes in the maintenance and repair of surface vessels.\n","date":1603872000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603872000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://charlielehman.github.io/author/charlie-lehman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/charlie-lehman/","section":"authors","summary":"Charlie Lehman is a Machine Learning Ph.D. student at the Georgia Tech Omni Lab fro Visual Engineering and Science (OLIVES). His research interests include robustness and explainability of deep vision models.","tags":null,"title":"Charlie Lehman","type":"authors"},{"authors":["Shirley Liu","Charlie Lehman","Ghassan AlRegib"],"categories":null,"content":"","date":1603872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603872000,"objectID":"0bce74dc2687976a5cf9e6e3f56ce5c2","permalink":"https://charlielehman.github.io/talk/icip2020-robustness-and-overfitting/","publishdate":"2020-10-22T00:00:00Z","relpermalink":"/talk/icip2020-robustness-and-overfitting/","section":"talk","summary":"SS-13:Explainable Machine Learning for Image Processing","tags":["Special Session","ICIP"],"title":"Robustness and Overfitting Behavior of Implicit Background Models","type":"talk"},{"authors":["Charlie Lehman","Can Temel","Ghassan AlRegib"],"categories":null,"content":"","date":1603791000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603791000,"objectID":"2a0668a14c53b249af142434fad12461","permalink":"https://charlielehman.github.io/talk/icip2020-on-the-structures/","publishdate":"2020-10-22T00:00:00Z","relpermalink":"/talk/icip2020-on-the-structures/","section":"talk","summary":"SS-08:Dynamic Background Reconstruction/Subtraction for Challenging Environments","tags":["Special Session","ICIP"],"title":"On the Structures of Representation for the Robustness of Semantic Segmentation to Input Corruption","type":"talk"},{"authors":["Moamen Soliman","Charlie Lehman","Ghassan AlRegib"],"categories":null,"content":"","date":1603789200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603789200,"objectID":"dbfc7667bd488dc660786c74bde7391f","permalink":"https://charlielehman.github.io/talk/icip2020-s6/","publishdate":"2020-10-22T00:00:00Z","relpermalink":"/talk/icip2020-s6/","section":"talk","summary":"ARS-14: Machine Learning for Image and Video Classification IV","tags":["Machine Learning","Semantic Segmentation","ICIP"],"title":"S6:Semi-Supervised Self-Supervised Semantic Segmentation","type":"talk"},{"authors":["Charlie Lehman","Can Temel","Ghassan AlRegib"],"categories":null,"content":"Introduction I wanted to provide some additional resources beyond the above linked paper and code to reproduce our results. Below is mostly the same code provided in the repository\u0026mdash;things like prints, form renderings for Google Colab, and rendered tqdm progress bars have been removed. Please be sure to expand the \u0026ldquo;show code \u0026gt;\u0026rdquo; buttons to look at the code that generated the corresponding outputs/results.\nGeneral Imports and Visualiztion methods We will be importing some of the required libraries here and build out the machinery for visualizing the results of this work. Of note in colorize_voc_label, is assigning the color white to classes above 20. This is to account for 255 being used to indicate the \u0026ldquo;ignore\u0026rdquo; class. For visualize, the assumption is the arguments are torch.Tensor instances that are the outputs of our models.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   import numpy as np import torch from functools import partial import os from PIL import Image import matplotlib %matplotlib inline from matplotlib import pyplot as plt from matplotlib import gridspec from matplotlib import cm from matplotlib.colors import ListedColormap, LinearSegmentedColormap def colorize_voc_label(lbl): voc_colors = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]] voc_colors = np.array(voc_colors)/255 voc_cmap = ListedColormap(voc_colors) cmap_lbl = voc_cmap(lbl/20) cmap_lbl[lbl\u0026gt;20,:3] = (1,1,1) return cmap_lbl def visualize(im, lbl, pred=None): im = im.permute(1,2,0).numpy() lbl = lbl.squeeze().numpy() cols = 3 if pred is not None else 2 fig, ax = plt.subplots(1,cols) im = ((im*MEAN_STD[\u0026#39;std\u0026#39;]+MEAN_STD[\u0026#39;mean\u0026#39;])*255).astype(np.uint8) ax[0].imshow(im) ax[0].set_title(\u0026#39;Image\u0026#39;) ax[1].imshow(colorize_voc_label(lbl)) ax[1].set_title(\u0026#39;Label\u0026#39;) ax[0].axis(\u0026#39;off\u0026#39;) ax[1].axis(\u0026#39;off\u0026#39;) if pred is not None: pred = pred.squeeze().numpy() ax[2].imshow(colorize_voc_label(pred)) ax[2].set_title(\u0026#39;Pred\u0026#39;) ax[2].axis(\u0026#39;off\u0026#39;) return fig, ax   Segmentation Transforms Now we build out the joint transform used during training and validation that is compatible with torchvision.datasets.VOCSegmentation. This requires the joint_transforms module provided in the above linked code.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from torchvision import transforms import joint_transforms MEAN_STD = {\u0026#34;mean\u0026#34;:(0.485, 0.456, 0.406), \u0026#34;std\u0026#34;:(0.229, 0.224, 0.225)} base_size = 224 crop_size = 224 class ImLblTransform(object): def __init__(self, train): self.joint_train = [] im_tran = [ transforms.ToTensor(), transforms.Normalize(**MEAN_STD) ] if train: self.joint_train.append(joint_transforms.RandomScaleCrop(base_size, crop_size, fill=255)) im_tran.insert(0,transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)) else: self.joint_train.append(joint_transforms.FixScaleResize(base_size)) self.img_transform = transforms.Compose(im_tran) def __call__(self, img, lbl): for tfm in self.joint_train: img, lbl = tfm(img, lbl) img = self.img_transform(img) lbl = np.array(lbl).astype(np.float32) lbl = torch.from_numpy(lbl).float() return img, lbl   VOC2012 Now to put our visualize method to work, we can look at the first image and label pair in the VOC2012 training set.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from torchvision.datasets import VOCSegmentation voc_train = VOCSegmentation(root=\u0026#39;/data/datasets/\u0026#39;, transforms=ImLblTransform(True)) voc_val = VOCSegmentation(root=\u0026#39;/data/datasets/\u0026#39;, transforms=ImLblTransform(False), image_set=\u0026#39;val\u0026#39;) im_,lbl_ = voc_train[0] fig, ax = visualize(im_,lbl_)   SBD Next, we want to append the VOC2012 training set with the \u0026ldquo;train_noval\u0026rdquo; subset of SBDataset as provided by torchvision to have a total of 7087 training examples. Notice as we visualize an example from SBDataset that they differ in that the transitions from foreground to background is not bordered by the \u0026ldquo;ignore\u0026rdquo; class.\n  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from torchvision.datasets import SBDataset sbd_train = SBDataset(root=\u0026#39;/data/datasets/SBD\u0026#39;, image_set=\u0026#39;train_noval\u0026#39;, mode=\u0026#39;segmentation\u0026#39;, transforms=ImLblTransform(True)) im_,lbl_ = sbd_train[0] fig, ax = visualize(im_,lbl_)   Dataloader Now to bring the datasets together for use in training and validation. If you will be implementing this yourself, note that we used a batch size of 20, which may exceed the memory available on your GPU.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from torch.utils import data vocsbd_train = data.ConcatDataset([voc_train, sbd_train]) train_iter = data.DataLoader(vocsbd_train, batch_size=20, shuffle=True, num_workers=12, pin_memory=True, drop_last=True) STEPS_PER_EPOCH = len(train_iter) print(\u0026#39;Steps per Epoch: {}\u0026#39;.format(STEPS_PER_EPOCH)) val_iter = data.DataLoader(voc_val, batch_size=1, shuffle=False, num_workers=12, pin_memory=True)   Steps per Epoch: 354  Corruptions Corruptions to Transform In order to use the machinery in pytorch we wrapped the corruption methods from the imagenet-c package in the standard pytorch transform interface. ImageNet-C is part of a collection of work on studying the impacts of corruptions by Dan Hendrycks, Thomas Dietterich, and others. More details can be found on the ImageNet-C Repository.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from imagenet_c import corrupt, corruption_tuple from functools import partial from itertools import product as iterprod from PIL import Image corr_dict = {} [corr_dict.update({p.__name__.split(\u0026#39;_\u0026#39;)[0]:n}) for n,p in enumerate(corruption_tuple[:15])] print(corr_dict) class ImLblCorruptTransform(object): def __init__(self, severity, corruption_number): corrupt_partial = partial(corrupt, severity=severity, corruption_number=corruption_number) self.joint_transform = joint_transforms.FixedResize(224) self.transform = lambda sz: transforms.Compose( [ np.array, corrupt_partial, Image.fromarray, transforms.Resize(sz), transforms.ToTensor(), transforms.Normalize(**MEAN_STD), ] ) if severity == 0: self.transform = lambda sz: transforms.Compose( [ transforms.ToTensor(), transforms.Normalize(**MEAN_STD) ] ) def __call__(self, img, lbl): img, lbl = self.joint_transform(img,lbl) W,H = img.size sz = (H,W) img = self.transform(sz)(img) lbl = np.array(lbl).astype(np.float32) lbl = torch.from_numpy(lbl).float() return img, lbl   { 'gaussian': 0, 'shot': 1, 'impulse': 2, 'defocus': 3, 'glass': 4, 'motion': 5, 'zoom': 6, 'snow': 7, 'frost': 8, 'fog': 9, 'brightness': 10, 'contrast': 11, 'elastic': 12, 'pixelate': 13, 'jpeg': 14 }  VOC Corruptions 4,5,6,7 Glass, Motion, Zoom and Snow take a long time for each iteration, so we can gain efficiencies by preprocessing these at all corruption levels. To do so, use the provided script dump_voc_c.py with the desired corruption number and severity.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from torchvision import datasets as dsets from torch.utils.data import Dataset, DataLoader from PIL import Image MEAN_STD = {\u0026#34;mean\u0026#34;:(0.485, 0.456, 0.406), \u0026#34;std\u0026#34;:(0.229, 0.224, 0.225)} class d_4567(Dataset): def __init__(self, cn, sv): name = corruption_tuple[cn].__name__ self.name = name self.sv = sv imgdir = \u0026#39;VOC-C/{}/{}/\u0026#39;.format(name,sv) lbldir = \u0026#39;VOC-C/lbl/\u0026#39; self.img_list = [imgdir+f for f in os.listdir(imgdir)] self.lbl_list = [lbldir+f for f in os.listdir(lbldir)] self.transform = transforms.Compose( [ transforms.ToTensor(), transforms.Normalize(**MEAN_STD) ] ) def __len__(self): return len(self.img_list) def __getitem__(self, idx): img = Image.open(self.img_list[idx]).convert(\u0026#39;RGB\u0026#39;) lbl = Image.open(self.lbl_list[idx]) img = self.transform(img) lbl = np.array(lbl).astype(np.float32) lbl = torch.from_numpy(lbl).float() return img, lbl def __str__(self): return \u0026#39;{} @ {}\u0026#39;.format(self.name, self.sv) d_ = d_4567(5,4) print(d_) im,lbl = d_[13] fig, ax = visualize(im,lbl)    motion_blur @ 4  Visualize Corruptions Here\u0026rsquo;s what the different corruption levels look like for a subset of the corruptions.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   index = 687#@param {type:\u0026#34;integer\u0026#34;} image_set = \u0026#34;val\u0026#34; #@param [\u0026#34;val\u0026#34;, \u0026#34;train\u0026#34;] from imagenet_c import corrupt, corruption_tuple from PIL import Image import numpy as np from matplotlib import gridspec dataset = VOCSegmentation(root=\u0026#39;/data/datasets/\u0026#39;, image_set=image_set) im_,lbl_ = dataset[index] w, h = im_.size new_h = 100 new_low_h = 30 new_w = 150 ratio = w/h s = 10 fig = plt.figure(figsize=(s*ratio, s)) gs1 = gridspec.GridSpec(5,5) gs1.update(wspace=0.05, hspace=0.05) orig_size = im_.size i = 0 for cn in [0,3,7,10,12]: for severity in [1,2,3,4,5]: corrim = Image.fromarray(corrupt(np.array(im_.resize((224,224),2)), severity=severity, corruption_number=cn)).resize(orig_size) #corrim2 = corrim.crop((w//2-new_w//2,new_h,w//2+new_w//2,h-new_low_h)) ax1 = plt.subplot(gs1[i]) ax1.set_xticklabels([]) ax1.set_yticklabels([]) ax1.grid(False) ax1.imshow(corrim) if severity==1: corname = str(corruption_tuple[cn].__name__).split(\u0026#39;_\u0026#39;)[0] ax1.set_ylabel(corname) if cn==0: ax1.set_title(severity) i+=1 display()   Metrics Running Confusion Matrix This will allow us to get metrics while running through with batches during training or in aggregate across the entire validation set.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   import torch.nn.functional as F from sklearn.metrics import confusion_matrix as conf_mat class RunningConfusionMatrix(object): def __init__(self, num_classes, ignore_class=None): super(RunningConfusionMatrix, self).__init__() self.num_classes = num_classes self.ignore_class = ignore_class self.reset() def __call__(self, prediction, target): prediction = prediction.view(-1) target = target.view(-1) if self.ignore_class is not None: prediction = prediction[target!=self.ignore_class] target = target[target!=self.ignore_class] prediction = prediction[prediction!=self.ignore_class] target = target[prediction!=self.ignore_class] prediction_oh = F.one_hot(prediction, self.num_classes).float() target_oh = F.one_hot(target.long(), self.num_classes).float() _cm = target_oh.permute(1,0).mm(prediction_oh).long().cpu() self.cm += _cm.cpu() @property def acc(self): return self.cm.diag()/self.cm.sum(0) @property def iou(self): tp = self.cm.diag() fp = self.cm.sum(0) - tp fn = self.cm.sum(1) - tp return tp/(tp+fp+fn) def reset(self): self.cm = torch.zeros(self.num_classes, self.num_classes)   Experiment Trainer Here is a configurable implementation of a semantic segmentation experiment.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from torchvision import utils from IPython.display import display, clear_output from tqdm.notebook import tqdm from torch import nn class SemanticSegmentation(object): def __init__(self, config): self.cuda = config[\u0026#39;cuda\u0026#39;] self.one_hot = config[\u0026#39;one_hot\u0026#39;] self.device = \u0026#39;cuda\u0026#39; if self.cuda else \u0026#39;cpu\u0026#39; self.confusion_matrix = RunningConfusionMatrix(config[\u0026#39;num_classes\u0026#39;], 255) model = config[\u0026#39;model\u0026#39;][\u0026#39;class\u0026#39;](**config[\u0026#39;model\u0026#39;][\u0026#39;kwargs\u0026#39;]) criterion = config[\u0026#39;criterion\u0026#39;][\u0026#39;class\u0026#39;](**config[\u0026#39;criterion\u0026#39;][\u0026#39;kwargs\u0026#39;]) self.optimizer = config[\u0026#39;optimizer\u0026#39;][\u0026#39;class\u0026#39;]([ {\u0026#39;params\u0026#39;:model.backbone.parameters()}, {\u0026#39;params\u0026#39;:model.classifier.parameters(), \u0026#39;lr\u0026#39;:config[\u0026#39;optimizer\u0026#39;][\u0026#39;kwargs\u0026#39;][\u0026#39;lr\u0026#39;]*10}, ],**config[\u0026#39;optimizer\u0026#39;][\u0026#39;kwargs\u0026#39;]) self.train_iter = config[\u0026#39;train_iter\u0026#39;][\u0026#39;class\u0026#39;](**config[\u0026#39;train_iter\u0026#39;][\u0026#39;kwargs\u0026#39;]) self.val_iter = config[\u0026#39;val_iter\u0026#39;][\u0026#39;class\u0026#39;](**config[\u0026#39;val_iter\u0026#39;][\u0026#39;kwargs\u0026#39;]) if self.cuda: #self.model = DataParallelModel(model.to(self.device), device_ids=[0,1]) #self.criterion = DataParallelCriterion(criterion.to(self.device), device_ids=[0,1]) self.model = nn.DataParallel(model, device_ids=[0,1]).cuda() self.criterion = criterion else: self.model = model self.criterion = criterion self.steps = 0 self.epoch_n = 0 self.config = config def evaluator(self): pass def step(self, input, target, oh_target): name = \u0026#39;Train\u0026#39; if self.model.training else \u0026#39;Val\u0026#39; output = self.model(input) pred = output.argmax(1) self.confusion_matrix(pred, target) iou = self.confusion_matrix.iou miou = iou[~torch.isnan(iou)].mean() if self.model.training: self.confusion_matrix.reset() loss = self.criterion(output, oh_target if self.one_hot else target.long()).mean() self.optimizer.zero_grad() loss.backward() self.optimizer.step() self.steps += 1 _loss = loss.item() else: _loss = 0 self.tbar.set_description(\u0026#39;[{} - {}] Loss: {:.3f}, mIOU: {:.3f}\u0026#39;.format(self.epoch_n, name, _loss, miou)) return pred.cpu() def epoch(self, data_iter): self.tbar = tqdm(data_iter) for input, target in self.tbar: _target = target.clone() _target[target==255] = 0 oh_target = nn.functional.one_hot(_target.long(),21).permute(0,3,1,2).float() pred = self.step(input.to(self.device), target.to(self.device), oh_target.to(self.device)) def train(self, num_epochs, lr_sched=True): if lr_sched: poly = lambda step: (1 - step/num_epochs)**0.9 else: poly = lambda step: 1 self.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=poly) for n in range(num_epochs): self.epoch_n = n self.model.train() self.epoch(self.train_iter) if self.config[\u0026#39;validate_while_train\u0026#39;]: self.validate() self.lr_scheduler.step() def validate(self): self.confusion_matrix.reset() self.model.eval() self.epoch(self.val_iter) def visualize(self, input, target, pred): fig, ax = visualize(input[0], target[0], pred[0])   Models Below we prepare three versions of the DeepLab v3+ with ResNet50 backbone: vanilla, Implicit Background Estimation (IBE), and Sigmoid Cross Entropy Implicit Background Estimation (SCrIBE). Since the models are trained from an ImageNet pretrained ResNet50, the appropriate layers are replaced and wrapped in an nn.Module. We then train each model using the previously introduced configurable experiment.\nDeepLabV3+  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from torchvision.models.segmentation import deeplabv3_resnet50 from torchvision.models import resnet50 from torchvision.models._utils import IntermediateLayerGetter from torch import nn class DLv3_ResNet50(nn.Module): def __init__(self, num_classes=21): super(DLv3_ResNet50, self).__init__() model = deeplabv3_resnet50(num_classes = num_classes) backbone = resnet50(pretrained=True, replace_stride_with_dilation=[False, True, True]) return_layers = {\u0026#39;layer4\u0026#39;: \u0026#39;out\u0026#39;} model.backbone = IntermediateLayerGetter(backbone, return_layers=return_layers) self.backbone = model.backbone self.classifier = model.classifier def forward(self,x): input_shape = x.shape[-2:] features = self.backbone(x) x = features[\u0026#34;out\u0026#34;] x = self.classifier(x) x = F.interpolate(x, size=input_shape, mode=\u0026#39;bilinear\u0026#39;, align_corners=False) return x ns_config = { \u0026#39;num_classes\u0026#39;:21, \u0026#39;one_hot\u0026#39;:False, \u0026#39;cuda\u0026#39;:True, \u0026#39;validate_while_train\u0026#39;:True, \u0026#39;model\u0026#39;:{ \u0026#39;class\u0026#39;:DLv3_ResNet50, \u0026#39;kwargs\u0026#39;:{ \u0026#39;num_classes\u0026#39;:21, }, }, \u0026#39;criterion\u0026#39;:{ \u0026#39;class\u0026#39;:nn.CrossEntropyLoss, \u0026#39;kwargs\u0026#39;:{ \u0026#39;ignore_index\u0026#39;:255, }, }, \u0026#39;optimizer\u0026#39;:{ \u0026#39;class\u0026#39;: torch.optim.SGD, \u0026#39;kwargs\u0026#39;:{ \u0026#39;lr\u0026#39;:0.01, \u0026#39;momentum\u0026#39;:0.9, \u0026#39;weight_decay\u0026#39;:5e-5, \u0026#39;nesterov\u0026#39;:False }, }, \u0026#39;train_iter\u0026#39;:{ \u0026#39;class\u0026#39;:data.DataLoader, \u0026#39;kwargs\u0026#39;:{ \u0026#39;dataset\u0026#39;:vocsbd_train, \u0026#39;batch_size\u0026#39;:30, \u0026#39;shuffle\u0026#39;:True, \u0026#39;num_workers\u0026#39;:12, \u0026#39;pin_memory\u0026#39;:True, \u0026#39;drop_last\u0026#39;:True, } }, \u0026#39;val_iter\u0026#39;:{ \u0026#39;class\u0026#39;:data.DataLoader, \u0026#39;kwargs\u0026#39;:{ \u0026#39;dataset\u0026#39;:voc_val, \u0026#39;batch_size\u0026#39;:1, \u0026#39;shuffle\u0026#39;:False, \u0026#39;num_workers\u0026#39;:1, \u0026#39;pin_memory\u0026#39;:False, \u0026#39;drop_last\u0026#39;:False, } } } pth = \u0026#39;data/DLv3_ResNet50.pth\u0026#39; if os.path.isfile(pth): no_scribe_model = ns_config[\u0026#39;model\u0026#39;][\u0026#39;class\u0026#39;](**ns_config[\u0026#39;model\u0026#39;][\u0026#39;kwargs\u0026#39;]) no_scribe_model.load_state_dict(torch.load(pth)) else: no_scribe_experiment = SemanticSegmentation(ns_config) no_scribe_experiment.train(50, True) no_scribe_model = no_scribe_experiment.model.module torch.save(no_scribe_model.state_dict(), pth) no_scribe_experiment.validate()   DeepLabV3+IBE  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   class DLv3_ResNet50_IBE(DLv3_ResNet50): def __init__(self, num_classes=20): super(DLv3_ResNet50_IBE, self).__init__(num_classes=num_classes) def forward(self,x): input_shape = x.shape[-2:] features = self.backbone(x) x = features[\u0026#34;out\u0026#34;] x = self.classifier(x) x = torch.cat([-torch.logsumexp(x,1, keepdim=True),x],1) x = F.interpolate(x, size=input_shape, mode=\u0026#39;bilinear\u0026#39;, align_corners=False) return x i_config = { \u0026#39;num_classes\u0026#39;:21, \u0026#39;one_hot\u0026#39;:False, \u0026#39;cuda\u0026#39;:True, \u0026#39;validate_while_train\u0026#39;:True, \u0026#39;model\u0026#39;:{ \u0026#39;class\u0026#39;:DLv3_ResNet50_IBE, \u0026#39;kwargs\u0026#39;:{ \u0026#39;num_classes\u0026#39;:20, }, }, \u0026#39;criterion\u0026#39;:{ \u0026#39;class\u0026#39;:nn.CrossEntropyLoss, \u0026#39;kwargs\u0026#39;:{ \u0026#39;ignore_index\u0026#39;:255, }, }, \u0026#39;optimizer\u0026#39;:{ \u0026#39;class\u0026#39;: torch.optim.SGD, \u0026#39;kwargs\u0026#39;:{ \u0026#39;lr\u0026#39;:0.01, \u0026#39;momentum\u0026#39;:0.9, \u0026#39;weight_decay\u0026#39;:5e-5, \u0026#39;nesterov\u0026#39;:False } }, \u0026#39;train_iter\u0026#39;:{ \u0026#39;class\u0026#39;:data.DataLoader, \u0026#39;kwargs\u0026#39;:{ \u0026#39;dataset\u0026#39;:vocsbd_train, \u0026#39;batch_size\u0026#39;:30, \u0026#39;shuffle\u0026#39;:True, \u0026#39;num_workers\u0026#39;:12, \u0026#39;pin_memory\u0026#39;:True, \u0026#39;drop_last\u0026#39;:True, } }, \u0026#39;val_iter\u0026#39;:{ \u0026#39;class\u0026#39;:data.DataLoader, \u0026#39;kwargs\u0026#39;:{ \u0026#39;dataset\u0026#39;:voc_val, \u0026#39;batch_size\u0026#39;:5, \u0026#39;shuffle\u0026#39;:False, \u0026#39;num_workers\u0026#39;:1, \u0026#39;pin_memory\u0026#39;:False, \u0026#39;drop_last\u0026#39;:False, } } } pth = \u0026#39;data/DLv3_IBE.pth\u0026#39; if os.path.isfile(pth): ibe_model = i_config[\u0026#39;model\u0026#39;][\u0026#39;class\u0026#39;](**i_config[\u0026#39;model\u0026#39;][\u0026#39;kwargs\u0026#39;]) ibe_model.load_state_dict(torch.load(pth)) else: ibe_experiment = SemanticSegmentation(i_config) ibe_experiment.train(50, True) ibe_model = ibe_experiment.model.module torch.save(ibe_model.state_dict(), pth) ibe_experiment.validate()    DeepLabV3+ScrIBE  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   class DLv3_ResNet50_SCrIBE(DLv3_ResNet50): def __init__(self, num_classes=20): super(DLv3_ResNet50_SCrIBE, self).__init__(num_classes=num_classes) def forward(self,x): input_shape = x.shape[-2:] _x = self.backbone(x) x = self.classifier(_x[\u0026#34;out\u0026#34;]) x = torch.cat([-torch.logsumexp(x,1, keepdim=True),x],1) x = F.interpolate(x, size=input_shape, mode=\u0026#39;bilinear\u0026#39;, align_corners=False) return x s_config = { \u0026#39;num_classes\u0026#39;:21, \u0026#39;one_hot\u0026#39;:True, \u0026#39;cuda\u0026#39;:True, \u0026#39;validate_while_train\u0026#39;:True, \u0026#39;model\u0026#39;:{ \u0026#39;class\u0026#39;:DLv3_ResNet50_SCrIBE, \u0026#39;kwargs\u0026#39;:{ \u0026#39;num_classes\u0026#39;:20, }, }, \u0026#39;criterion\u0026#39;:{ \u0026#39;class\u0026#39;:nn.BCEWithLogitsLoss, \u0026#39;kwargs\u0026#39;:{ }, }, \u0026#39;optimizer\u0026#39;:{ \u0026#39;class\u0026#39;: torch.optim.SGD, \u0026#39;kwargs\u0026#39;:{ \u0026#39;lr\u0026#39;:0.01, \u0026#39;momentum\u0026#39;:0.9, \u0026#39;weight_decay\u0026#39;:5e-5, \u0026#39;nesterov\u0026#39;:False } }, \u0026#39;train_iter\u0026#39;:{ \u0026#39;class\u0026#39;:data.DataLoader, \u0026#39;kwargs\u0026#39;:{ \u0026#39;dataset\u0026#39;:vocsbd_train, \u0026#39;batch_size\u0026#39;:30, \u0026#39;shuffle\u0026#39;:True, \u0026#39;num_workers\u0026#39;:12, \u0026#39;pin_memory\u0026#39;:True, \u0026#39;drop_last\u0026#39;:True, } }, \u0026#39;val_iter\u0026#39;:{ \u0026#39;class\u0026#39;:data.DataLoader, \u0026#39;kwargs\u0026#39;:{ \u0026#39;dataset\u0026#39;:voc_val, \u0026#39;batch_size\u0026#39;:1, \u0026#39;shuffle\u0026#39;:False, \u0026#39;num_workers\u0026#39;:1, \u0026#39;pin_memory\u0026#39;:False, \u0026#39;drop_last\u0026#39;:False, } } } pth = \u0026#39;data/DLv3_SCrIBE.pth\u0026#39; if os.path.isfile(pth): scribe_model = s_config[\u0026#39;model\u0026#39;][\u0026#39;class\u0026#39;](**s_config[\u0026#39;model\u0026#39;][\u0026#39;kwargs\u0026#39;]) scribe_model.load_state_dict(torch.load(pth)) else: scribe_experiment = SemanticSegmentation(s_config) scribe_experiment.train(50, True) scribe_model = scribe_experiment.model.module torch.save(scribe_model.state_dict(), pth) scribe_experiment.validate()    Representation Metrics Running Logit Tracker Much like the Running Confusion Matrix, we will also track the logits or pre-softmax model outputs over a run of batched iterations for later analysis.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   class RunningLogitTracker(object): def __init__(self, num_classes, ignore_class=None): super(RunningLogitTracker, self).__init__() self.num_classes = num_classes self.ignore_class = ignore_class self.reset() def __call__(self, output): output = output.permute(0,2,3,1).reshape(-1,self.num_classes) _pred = output.argmax(1) for n in range(self.num_classes): x = output[_pred==n,:].detach() self.counts[n] += x.size(0) self.sums[n] += x.sum(0).cpu() self.sumsqs[n] += x.permute(1,0).mm(x).cpu() def dist(self,x,y): return (x-y).pow(2).sum().sqrt() @property def dm(self): _dm = torch.zeros(self.num_classes, self.num_classes) mn = self.mean for i in range(0,self.num_classes): for j in range(i,self.num_classes): _dm[i,j] = self.dist(mn[i], mn[j]) _dm[j,i] = _dm[i,j] return _dm @property def mean(self): out = self.sums/self.counts.unsqueeze(1) out[out!=out] = 0 return out @property def cov(self): covs = [] for n in range(self.num_classes): mn = self.mean[n].unsqueeze(0) sq = self.sumsqs[n] _cov = (sq - mn.permute(1,0).mm(mn))/self.counts[n] _cov[_cov!=_cov] = 0 covs.append(_cov) return torch.stack(covs,0) @property def cor(self): covs = self.cov cors = [] for n in range(self.num_classes): S = covs[n] Dinv = torch.inverse(S.diag().diag().sqrt()) R = Dinv.mm(S).mm(Dinv) cors.append(R) return torch.stack(cors,0) def reset(self): self.counts = torch.zeros(self.num_classes) self.sums = torch.zeros(self.num_classes, self.num_classes) self.sumsqs = torch.zeros(self.num_classes, self.num_classes, self.num_classes)   Run over all Corruptions and Levels Here we measure the performance of each model for each corruption at each level. This also takes a while, but has some progress saving built in.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   import torch import gc import pandas as pd batch = 20 nm = \u0026#39;data/DistCombined.pkl\u0026#39; scribe_model = scribe_model.to(0) ibe_model = ibe_model.to(0) no_scribe_model = no_scribe_model.to(1) scribe_model.eval() ibe_model.eval() no_scribe_model.eval() try: dist_df = pd.read_pickle(nm).drop_duplicates() lgst_cn = dist_df[\u0026#39;corruption_number\u0026#39;].max() lgst_sv = dist_df[dist_df[\u0026#39;corruption_number\u0026#39;]==lgst_cn][\u0026#39;Severity\u0026#39;].max() print(\u0026#39;Restarting from {}@{}\u0026#39;.format(lgst_cn, lgst_sv)) dist_data = dist_df.to_dict(\u0026#39;records\u0026#39;) flag = False except: print(\u0026#39;New Run!\u0026#39;) lgst_cn = 0 lgst_sv = 0 dist_data = [] flag = False for cn in range(lgst_cn,15): corruption_name = corruption_tuple[cn].__name__ for sv in range(6): if sv==0 and flag: print(\u0026#39;Case 1: Skipping {}@{}\u0026#39;.format(cn, sv)) continue if cn != lgst_cn: lgst_sv=-1 if sv \u0026lt; lgst_sv: print(\u0026#39;Case 2: Skipping {}@{}\u0026#39;.format(cn, sv)) continue s_cm = RunningConfusionMatrix(21, 255) i_cm = RunningConfusionMatrix(21, 255) n_cm = RunningConfusionMatrix(21, 255) s_lt = RunningLogitTracker(21, 255) i_lt = RunningLogitTracker(21, 255) n_lt = RunningLogitTracker(21, 255) if cn in [4,5,6,7]: corr_val = d_4567(cn,sv) else: corr_val = VOCSegmentation(root=\u0026#39;/data/datasets/\u0026#39;, transforms=ImLblCorruptTransform(sv,cn), image_set=\u0026#39;val\u0026#39;) corr_iter = data.DataLoader(corr_val, batch_size=batch, shuffle=False, num_workers=1, pin_memory=True) pbar = tqdm(corr_iter, position=0, leave=True) for im,lbl in pbar: s_output = scribe_model(im.to(0)) i_output = ibe_model(im.to(0)) n_output = no_scribe_model(im.to(1)) s_pred = s_output.argmax(1) i_pred = i_output.argmax(1) n_pred = n_output.argmax(1) s_cm(s_pred, lbl.to(0)) i_cm(i_pred, lbl.to(0)) n_cm(n_pred, lbl.to(1)) s_iou = s_cm.iou s_miou = s_iou[~torch.isnan(s_iou)].mean() i_iou = i_cm.iou i_miou = i_iou[~torch.isnan(i_iou)].mean() n_iou = n_cm.iou n_miou = n_iou[~torch.isnan(n_iou)].mean() s_lt(s_output) i_lt(i_output) n_lt(n_output) pbar.set_description(\u0026#39;{}@{} S/I/B: {:.3f} / {:.3f} / {:.3f}\u0026#39;.format(cn, sv, s_miou,i_miou,n_miou)) s_mn = s_lt.mean.numpy() i_mn = i_lt.mean.numpy() n_mn = n_lt.mean.numpy() s_r = np.mean(np.diagonal(s_mn)[1:]-s_mn[1:,0]) i_r = np.mean(np.diagonal(i_mn)[1:]-i_mn[1:,0]) n_r = np.mean(np.diagonal(n_mn)[1:]-n_mn[1:,0]) dist_data.append( { \u0026#39;Model\u0026#39;:\u0026#39;ScrIBE\u0026#39;, \u0026#39;Corruption\u0026#39;:corruption_name, \u0026#39;corruption_number\u0026#39;:cn, \u0026#39;Severity\u0026#39;:sv, \u0026#39;mIOU\u0026#39;:s_miou.item(), \u0026#39;Distance\u0026#39;:s_r } ) dist_data.append( { \u0026#39;Model\u0026#39;:\u0026#39;IBE\u0026#39;, \u0026#39;Corruption\u0026#39;:corruption_name, \u0026#39;corruption_number\u0026#39;:cn, \u0026#39;Severity\u0026#39;:sv, \u0026#39;mIOU\u0026#39;:i_miou.item(), \u0026#39;Distance\u0026#39;:i_r } ) dist_data.append( { \u0026#39;Model\u0026#39;:\u0026#39;Baseline\u0026#39;, \u0026#39;Corruption\u0026#39;:corruption_name, \u0026#39;corruption_number\u0026#39;:cn, \u0026#39;Severity\u0026#39;:sv, \u0026#39;mIOU\u0026#39;:n_miou.item(), \u0026#39;Distance\u0026#39;:n_r } ) dist_df = pd.DataFrame(dist_data) dist_df.to_pickle(nm) flag = True   Restarting from 14@5 Case 2: Skipping 14@0 Case 2: Skipping 14@1 Case 2: Skipping 14@2 Case 2: Skipping 14@3 Case 2: Skipping 14@4  Run Validation  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   import torch import gc import pandas as pd torch.cuda.empty_cache() def run_one(scribe_model, ibe_model, no_scribe_model): scribe_model = scribe_model.to(0) ibe_model = ibe_model.to(0) no_scribe_model = no_scribe_model.to(1) scribe_model.eval() ibe_model.eval() no_scribe_model.eval() s_cm = RunningConfusionMatrix(21, 255) i_cm = RunningConfusionMatrix(21, 255) n_cm = RunningConfusionMatrix(21, 255) s_lt = RunningLogitTracker(21, 255) i_lt = RunningLogitTracker(21, 255) n_lt = RunningLogitTracker(21, 255) corr_val = VOCSegmentation(root=\u0026#39;/data/datasets/\u0026#39;, transforms=ImLblTransform(False), image_set=\u0026#39;val\u0026#39;) corr_iter = data.DataLoader(corr_val, batch_size=1, shuffle=False, num_workers=1, pin_memory=True) pbar = tqdm(corr_iter, position=0, leave=True) for im,lbl in pbar: s_output = scribe_model(im.to(0)) i_output = ibe_model(im.to(0)) n_output = no_scribe_model(im.to(1)) s_pred = s_output.argmax(1) i_pred = i_output.argmax(1) n_pred = n_output.argmax(1) s_cm(s_pred, lbl.to(0)) i_cm(i_pred, lbl.to(0)) n_cm(n_pred, lbl.to(1)) s_iou = s_cm.iou s_miou = s_iou[~torch.isnan(s_iou)].mean() i_iou = i_cm.iou i_miou = i_iou[~torch.isnan(i_iou)].mean() n_iou = n_cm.iou n_miou = n_iou[~torch.isnan(n_iou)].mean() s_lt(s_output) i_lt(i_output) n_lt(n_output) pbar.set_description(\u0026#39;S/I/B: {:.3f} / {:.3f} / {:.3f}\u0026#39;.format(s_miou,i_miou,n_miou)) return s_lt, i_lt, n_lt s_lt, i_lt, n_lt = run_one(scribe_model, ibe_model, no_scribe_model)   Dimensionality Analysis Explained Variance  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from sklearn.decomposition import PCA import seaborn as sns import pandas as pd sns.set_style(\u0026#34;whitegrid\u0026#34;) sns.set_context(\u0026#34;paper\u0026#34;, font_scale=1.5, rc={\u0026#34;lines.linewidth\u0026#34;: 2.5}) pca_data = [] pca = PCA(n_components=21) pca.fit(s_lt.mean) s_cs = pca.explained_variance_ratio_.cumsum() pca = PCA(n_components=21) pca.fit(i_lt.mean) i_cs = pca.explained_variance_ratio_.cumsum() pca = PCA(n_components=21) pca.fit(n_lt.mean) n_cs = pca.explained_variance_ratio_.cumsum() for m, p in zip([\u0026#39;SCrIBE\u0026#39;, \u0026#39;IBE\u0026#39;, \u0026#39;Baseline\u0026#39;],[s_cs, i_cs, n_cs]): for x,y in enumerate(p): _d = { \u0026#39;Model\u0026#39;: m, \u0026#39;Variant\u0026#39;: \u0026#39;Single\u0026#39;, \u0026#39;Component\u0026#39;:x, \u0026#39;Explained Variance\u0026#39;:y } pca_data.append(_d) pca_df = pd.DataFrame(pca_data) fig, ax = plt.subplots(figsize=[10,3]) sns.lineplot(ax=ax,data=pca_df, x=\u0026#39;Component\u0026#39;, y=\u0026#39;Explained Variance\u0026#39;, hue=\u0026#39;Model\u0026#39;, hue_order =[\u0026#39;Baseline\u0026#39;, \u0026#39;IBE\u0026#39;, \u0026#39;SCrIBE\u0026#39;])   Structural Analysis  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from torchvision.utils import make_grid from matplotlib.colors import LogNorm import seaborn as sns from mpl_toolkits.axes_grid1 import make_axes_locatable sns.set(style=\u0026#34;whitegrid\u0026#34;) sns.set_context(\u0026#34;poster\u0026#34;, font_scale=1.5, rc={\u0026#34;lines.linewidth\u0026#34;: 2.5}) fig, ax = plt.subplots(1,3, figsize=(14,4)) s_mn = s_lt.mean.numpy() i_mn = i_lt.mean.numpy() n_mn = n_lt.mean.numpy() s_r = np.diagonal(s_mn)-s_mn[:,0] i_r = np.diagonal(i_mn)-i_mn[:,0] n_r = np.diagonal(n_mn)-n_mn[:,0] _mx = max([s_mn[s_mn!=-255].max(), i_mn[i_mn!=-255].max(), n_mn[n_mn!=-255].max()]) _mn = min([s_mn[s_mn!=-255].min(), i_mn[i_mn!=-255].min(), n_mn[n_mn!=-255].min()]) im = ax[0].imshow(s_mn, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[1].imshow(i_mn, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[2].imshow(n_mn, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[0].axis(\u0026#39;off\u0026#39;) ax[1].axis(\u0026#39;off\u0026#39;) ax[2].axis(\u0026#39;off\u0026#39;) cbar_ax = fig.add_axes([0.95, 0.15, 0.05, 0.7]) fig.colorbar(im, cax=cbar_ax) plt.show() fig, ax = plt.subplots(1,3, figsize=(14,4)) s_mn = s_lt.dm.numpy() i_mn = i_lt.dm.numpy() n_mn = n_lt.dm.numpy() _mx = max([s_mn[s_mn!=-255].max(), i_mn[i_mn!=-255].max(), n_mn[n_mn!=-255].max()]) _mn = min([s_mn[s_mn!=-255].min(), i_mn[i_mn!=-255].min(), n_mn[n_mn!=-255].min()]) im = ax[0].imshow(s_mn, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[1].imshow(i_mn, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[2].imshow(n_mn, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[0].axis(\u0026#39;off\u0026#39;) ax[1].axis(\u0026#39;off\u0026#39;) ax[2].axis(\u0026#39;off\u0026#39;) cbar_ax = fig.add_axes([0.95, 0.15, 0.05, 0.7]) fig.colorbar(im, cax=cbar_ax) plt.show() s_im = make_grid(s_lt.cor.unsqueeze(1), nrow=3, padding=2, pad_value=-255)[0,:,:].numpy() i_im = make_grid(i_lt.cor.unsqueeze(1), nrow=3, padding=2, pad_value=-255)[0,:,:].numpy() n_im = make_grid(n_lt.cor.unsqueeze(1), nrow=3, padding=2, pad_value=-255)[0,:,:].numpy() _mx = max([s_im[s_im!=-255].max(), n_im[n_im!=-255].max()]) _mn = min([s_im[s_im!=-255].min(), n_im[n_im!=-255].min()]) print(_mn,_mx) fig, ax = plt.subplots(1,3, figsize=(14,9)) im = ax[0].imshow(s_im, vmin=0.8, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) im = ax[1].imshow(i_im, vmin=0.8, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[2].imshow(n_im, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[0].axis(\u0026#39;off\u0026#39;) ax[1].axis(\u0026#39;off\u0026#39;) ax[2].axis(\u0026#39;off\u0026#39;) cbar_ax = fig.add_axes([0.95, 0.15, 0.05, 0.7]) fig.colorbar(im, cax=cbar_ax)    .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   sns.set(style=\u0026#34;whitegrid\u0026#34;) sns.set_context(\u0026#34;paper\u0026#34;, font_scale=2.0, rc={\u0026#34;lines.linewidth\u0026#34;: 2.5}) s_im = make_grid(s_lt.cor[-8:-7].unsqueeze(1), nrow=3, padding=2, pad_value=-255)[0,:,:].numpy() i_im = make_grid(i_lt.cor[-8:-7].unsqueeze(1), nrow=3, padding=2, pad_value=-255)[0,:,:].numpy() n_im = make_grid(n_lt.cor[-8:-7].unsqueeze(1), nrow=3, padding=2, pad_value=-255)[0,:,:].numpy() s_mn = s_lt.mean.numpy() i_mn = i_lt.mean.numpy() n_mn = n_lt.mean.numpy() _mn, _mx = -1,1 fig, ax = plt.subplots(1,3, figsize=(10,3)) ax[0].set_title(\u0026#39;SCrIBE\u0026#39;) im1 = ax[0].imshow(s_im, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[1].set_title(\u0026#39;IBE\u0026#39;) im2 = ax[1].imshow(i_im, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[2].set_title(\u0026#39;Baseline\u0026#39;) im3 = ax[2].imshow(n_im, vmin=_mn, vmax=_mx, cmap=\u0026#39;plasma\u0026#39;) ax[0].axis(\u0026#39;off\u0026#39;) ax[1].axis(\u0026#39;off\u0026#39;) ax[2].axis(\u0026#39;off\u0026#39;) cbar_ax = fig.add_axes([0.95, 0.15, 0.04, 0.70]) fig.colorbar(im3, cax=cbar_ax) plt.show()   Qualitative Result Visualizations Make a list of images from Validation set  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   from joint_transforms import FixedResize class ImViewLblTransform(object): def __init__(self): im_tran = [ transforms.ToTensor(), transforms.Normalize(**MEAN_STD) ] self.joint_train = FixedResize(224) self.img_transform = transforms.Compose(im_tran) def __call__(self, img, lbl): img, lbl = self.joint_train(img, lbl) img = self.img_transform(img) lbl = np.array(lbl).astype(np.float32) lbl = torch.from_numpy(lbl).float() return img, lbl voc_val = VOCSegmentation(root=\u0026#39;/data/datasets/\u0026#39;, transforms=ImViewLblTransform(), image_set=\u0026#39;val\u0026#39;) val_set_list = [] for im, lbl in list(voc_val): im = im*torch.tensor(MEAN_STD[\u0026#39;std\u0026#39;]).reshape(3,1,1)+torch.tensor(MEAN_STD[\u0026#39;mean\u0026#39;]).reshape(3,1,1) val_set_list.append(im)   Render 100 of them starting at some index  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   delta = 700#@param {type:\u0026#34;integer\u0026#34;} from torchvision.utils import make_grid import matplotlib.patheffects as path_effects img = make_grid(torch.stack(val_set_list[0+delta:100+delta]), nrow=10, padding=1).permute(1,2,0).detach().cpu().numpy() fig = plt.figure(figsize=(20,20)) ax = fig.add_axes([0,0,1,1]) plt.imshow(img) n = 0 for i in range(10): for j in range(10): txt = ax.text(.05+j/10, .95-i/10, n+delta, horizontalalignment=\u0026#39;center\u0026#39;, verticalalignment=\u0026#39;center\u0026#39;, transform=ax.transAxes ) txt.set_path_effects([path_effects.Stroke(linewidth=3, foreground=\u0026#39;white\u0026#39;), path_effects.Normal()]) n += 1 plt.axis(\u0026#39;off\u0026#39;) plt.show()   Generate Results to Visualize Here we pick one from the group above and collect outputs for all models and corruptions at 3 levels for visualization. Crop top and crop bottom allow for adjusting the very tall figure.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   dataset_index = 790#@param {type:\u0026#34;number\u0026#34;} crop_top = 50#@param {type:\u0026#34;number\u0026#34;} crop_bot = 1#@param {type:\u0026#34;number\u0026#34;} Dataset = \u0026#34;val\u0026#34; #@param [\u0026#34;val\u0026#34;, \u0026#34;train\u0026#34;] from torchvision.utils import make_grid from matplotlib.figure import figaspect preds=[] scribe_model = scribe_model.to(0) no_scribe_model = no_scribe_model.to(1) scribe_model.eval() no_scribe_model.eval() corr_disp_list = [0,3,7,10,12] sv_list = range(0,4) for cn in corr_disp_list: for sv in sv_list: voc_val = VOCSegmentation(root=\u0026#39;/data/datasets/\u0026#39;, transforms=ImLblCorruptTransform(sv,cn), image_set=Dataset) im, lbl = voc_val[dataset_index] c,h,w = im.shape output = scribe_model(im.unsqueeze(0).to(0)) pred = output.argmax(1).cpu().squeeze().numpy() pred = torch.tensor(colorize_voc_label(pred)[:,:,:3]).float().permute(2,0,1) noutput = no_scribe_model(im.unsqueeze(0).to(1)) npred = noutput.argmax(1).cpu().squeeze().numpy() npred = torch.tensor(colorize_voc_label(npred)[:,:,:3]).float().permute(2,0,1) lbl= torch.tensor(colorize_voc_label(lbl)[:,:,:3]).float().permute(2,0,1) im = im*torch.tensor(MEAN_STD[\u0026#39;std\u0026#39;]).reshape(3,1,1)+torch.tensor(MEAN_STD[\u0026#39;mean\u0026#39;]).reshape(3,1,1) im = im[:,crop_top:-crop_bot,:] pred = pred[:,crop_top:-crop_bot,:] npred = npred[:,crop_top:-crop_bot,:] lbl = lbl[:,crop_top:-crop_bot,:] imp = torch.cat([im,npred,pred],1) preds.append(imp.detach().cpu()) if sv==0: or_img = im.permute(1,2,0).detach().cpu().numpy() or_lbl = lbl.permute(1,2,0).detach().cpu().numpy() sm_lbl = nn.functional.interpolate(lbl.unsqueeze(0), scale_factor=.4).squeeze() sl_c,sl_h,sl_w = sm_lbl.shape oi_c,oi_h,oi_w = lbl.shape mod_img = im.clone() mod_img[:,oi_h-sl_h:,oi_w-sl_w:] = sm_lbl pristine = make_grid(torch.stack([mod_img,npred,pred]),nrow=3,padding=0) pristine_ = pristine.permute(1,2,0).detach().cpu().numpy()    Visualize that collections This is the visualization code used to generate a figure in the paper.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   preds = torch.stack(preds) preds = make_grid(preds,nrow=len(sv_list),padding=0) disp_im = preds.permute(1,2,0).numpy()[0:len(corr_disp_list)*h*3,1*w:] d_h,d_w,_ = disp_im.shape p_h,p_w,_ = pristine_.shape p_w = d_w*3/(len(sv_list)-1) p_h *= d_w/p_w d_h /= 96 d_w /= 96 p_h /= 96 p_w /= 96 tot_h = (d_h+p_h)/0.7 tot_w = (d_w+p_w)/0.8 plt.rcParams.update({\u0026#39;font.size\u0026#39;: 18}) fig = plt.figure(figsize=(tot_w,tot_h)) ax_ = fig.add_axes([0.1,d_h/tot_h+0.12,0.8,p_h/tot_h]) ax_.axis(\u0026#39;off\u0026#39;) ax_.imshow(pristine_) ax_.text(1/(2*3)+1/3*0,1, \u0026#39;Input\u0026#39;, horizontalalignment=\u0026#39;center\u0026#39;, verticalalignment=\u0026#39;bottom\u0026#39;, transform=ax_.transAxes ) ax_.text(1/(2*3)+1/3*1,1, \u0026#39;Baseline\u0026#39;, horizontalalignment=\u0026#39;center\u0026#39;, verticalalignment=\u0026#39;bottom\u0026#39;, transform=ax_.transAxes ) ax_.text(1/(2*3)+1/3*1,1.15, \u0026#39;Original\u0026#39;, horizontalalignment=\u0026#39;center\u0026#39;, verticalalignment=\u0026#39;bottom\u0026#39;, transform=ax_.transAxes, fontweight=\u0026#39;bold\u0026#39; ) ax_.text(1/(2*3)+1/3*2,1, \u0026#39;SCrIBE\u0026#39;, horizontalalignment=\u0026#39;center\u0026#39;, verticalalignment=\u0026#39;bottom\u0026#39;, transform=ax_.transAxes ) ax = fig.add_axes([0.1,0.1,0.8,d_h/tot_h]) ax.axis(\u0026#39;off\u0026#39;) n_corr = len(corr_disp_list) y_start = (1-1/(2*n_corr*3)) y_step = 1/(n_corr) ax.imshow(disp_im) for j, cn in enumerate(corr_disp_list): nm = corruption_tuple[cn].__name__.split(\u0026#39;_\u0026#39;)[0].capitalize() ax.text(-.01,y_start-y_step*j, nm, horizontalalignment=\u0026#39;right\u0026#39;, verticalalignment=\u0026#39;center\u0026#39;, transform=ax.transAxes, rotation=90, fontweight=\u0026#39;bold\u0026#39; ) ax.text(-.01,y_start-y_step/3-y_step*j, \u0026#39;Baseline\u0026#39;, horizontalalignment=\u0026#39;right\u0026#39;, verticalalignment=\u0026#39;center\u0026#39;, rotation=90, transform=ax.transAxes ) ax.text(-.01,y_start-2*y_step/3-y_step*j, \u0026#39;SCrIBE\u0026#39;, horizontalalignment=\u0026#39;right\u0026#39;, verticalalignment=\u0026#39;center\u0026#39;, rotation=90, transform=ax.transAxes ) x_start = 1/(2*(len(sv_list)-1)) x_step = 1/(len(sv_list)-1) for sv in sv_list[1:]: ax.text(x_start+x_step*(sv-1),1, sv, horizontalalignment=\u0026#39;center\u0026#39;, verticalalignment=\u0026#39;bottom\u0026#39;, transform=ax.transAxes ) ax.text(x_start+x_step*(2-1),1.01, \u0026#39;Corrupted\u0026#39;, horizontalalignment=\u0026#39;center\u0026#39;, verticalalignment=\u0026#39;bottom\u0026#39;, transform=ax.transAxes, fontweight=\u0026#39;bold\u0026#39; ) display()   Performance Comparison Plot  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   import matplotlib as mpl import seaborn as sns import pandas as pd sns.set_style(\u0026#34;whitegrid\u0026#34;) sns.set_context(\u0026#34;poster\u0026#34;) df = pd.read_pickle(\u0026#39;data/DistCombined.pkl\u0026#39;) rgbs = [(213/255, 94/255, 0/255), (86/255, 180/255, 233/255), (.9, .9, .9)] cblind = [mpl.colors.to_hex(r) for r in rgbs] cblind_gray = [mpl.colors.to_hex( mpl.colors.hsv_to_rgb(mpl.colors.rgb_to_hsv(r) * (1,0,1))) for r in rgbs] pal = sns.color_palette(cblind) sns.palplot(pal) fig, ax = plt.subplots(figsize=(9*1.618,7)) sns.lineplot(x=\u0026#39;Severity\u0026#39;, y=\u0026#39;mIOU\u0026#39;, data=df, ax=ax, hue=\u0026#39;Model\u0026#39;) #plt.legend(bbox_to_anchor=(1.05,1), loc=2, borderaxespad=0.)   Example Videos Load the video Here we provide the code that was used to produce the introduction demo that was used in the presentation video.  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   import skvideo.io video = \u0026#39;walkAS.mp4\u0026#39; videodata = skvideo.io.vread(f\u0026#39;example_videos/{video}\u0026#39;)    Prepare the corrupting transform  .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   class ImCorruptTransform(object): def __init__(self, severity, corruption_number, red_size): corrupt_partial = partial(corrupt, severity=severity, corruption_number=corruption_number) self.transform = lambda sz: transforms.Compose( [ np.array, corrupt_partial, Image.fromarray, transforms.Resize(sz), transforms.ToTensor(), transforms.Normalize(**MEAN_STD), ] ) if severity == 0: self.transform = lambda sz: transforms.Compose( [ transforms.Resize(sz), transforms.ToTensor(), transforms.Normalize(**MEAN_STD) ] ) self.red_size = red_size def __call__(self, img): img = Image.fromarray(img) img = img.resize(self.red_size) W,H = img.size sz = (H,W) img = img.resize((224,224),Image.BILINEAR) img = self.transform(sz)(img) return img    Generate the demo video Here we equally divide the frames amongst corruptions and processing them through SCrIBE and the baseline models.\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   ns_preds = [] s_preds = [] ims = [] corr_ims = [] corr_txts = [] switch = videodata.shape[0]//13 sv = 0 cn =0 pbar = tqdm(enumerate(videodata), total=videodata.shape[0]) for k, im in pbar: if k%switch==0 and k!=0: sv = 2 cn += 1 if cn == 8: cn += 1 if cn == 12: sv = 3 if cn \u0026gt; 0: corr_txts.append(corruption_tuple[cn].__name__) else: corr_txts.append(\u0026#39;None\u0026#39;) pbar.set_description(corr_txts[-1]) transform = ImCorruptTransform(sv,cn, (480,270)) ims.append(np.array(transforms.Resize(270)(Image.fromarray(im)))) im = transform(im) corr_ims.append(im.permute([1,2,0]).numpy()) im = im.unsqueeze(0) s_pred = scribe_model(im.to(0)) s_pred = s_pred.argmax(1).cpu().numpy() s_c_out = colorize_voc_label(s_pred) s_preds.append(s_c_out[:,:,:,:3]) ns_pred = no_scribe_model(im.to(1)) ns_pred = ns_pred.argmax(1).cpu().numpy() ns_c_out = colorize_voc_label(ns_pred) ns_preds.append(ns_c_out[:,:,:,:3]) ns_preds = np.concatenate(ns_preds) s_preds = np.concatenate(s_preds)   Animate This takes a while. I am sure there is a faster way\u0026hellip;\n .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   import matplotlib.pyplot as plt from matplotlib import animation, rc rc(\u0026#39;animation\u0026#39;, html=\u0026#39;html5\u0026#39;) print(ims[0].shape, corr_ims[0].shape) def join_ex(im,corrim,ns_pred,s_pred): im = im/255.0 corrim = corrim*MEAN_STD[\u0026#39;std\u0026#39;]+MEAN_STD[\u0026#39;mean\u0026#39;] corrim = np.clip(corrim,0,1) top = np.concatenate([im, corrim],1) bot = np.concatenate([ns_pred, s_pred],1) return np.concatenate([top,bot], 0) my_dpi = 960 fig, ax = plt.subplots(1,figsize=(1920/my_dpi, 1080/my_dpi), dpi=my_dpi) vis = ax.imshow(join_ex(ims[0],corr_ims[0], ns_preds[0], s_preds[0])) corrtxt = ax.text(500, 30, f\u0026#39;Corruption: None\u0026#39;, fontsize=3, color=\u0026#39;red\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) corrtxt.set_path_effects([path_effects.Stroke(linewidth=1, foreground=\u0026#39;black\u0026#39;), path_effects.Normal()]) theirs = ax.text(20, 300, f\u0026#39;Theirs\u0026#39;, fontsize=3, color=\u0026#39;red\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) theirs.set_path_effects([path_effects.Stroke(linewidth=1, foreground=\u0026#39;black\u0026#39;), path_effects.Normal()]) ours = ax.text(500, 300, f\u0026#39;Ours\u0026#39;, fontsize=3, color=\u0026#39;red\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ours.set_path_effects([path_effects.Stroke(linewidth=1, foreground=\u0026#39;black\u0026#39;), path_effects.Normal()]) def animate(i): corrtxt.set_text(f\u0026#39;Corruption: {corr_txts[i]}\u0026#39;) vis.set_array(join_ex(ims[i],corr_ims[i], ns_preds[i], s_preds[i])) return [vis] def init(): vis.set_array(join_ex(ims[0],corr_ims[0], ns_preds[0], s_preds[0])) return [vis] fig.tight_layout() fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None) ax.set_axis_off() ani = animation.FuncAnimation(fig, animate, frames=s_preds.shape[0], interval=30, blit=True, init_func=init)    (270, 480, 3) (270, 480, 3)   .definition { visibility: hidden; width: 120px; background-color: #555; color: #fff; text-align: center; border-radius: 6px; padding: 5px 0; position: absolute; z-index: 1; bottom: 125%; left: 50%; margin-left: -60px; opacity: 0; transition: opacity 0.3s; } .term { position: relative; display: inline-block; border-bottom: 1px dotted black;} .term:hover .definition { visibility: visible; opacity: 1; }  show code   # Set up formatting for the movie files Writer = animation.writers[\u0026#39;ffmpeg\u0026#39;] writer = Writer(fps=30, metadata=dict(artist=\u0026#39;Me\u0026#39;), bitrate=18000) ani.save(f\u0026#39;example_videos/scribe_pred_{video}\u0026#39;, writer=writer)   Thank you for making it to the bottom of this post. I hope you will feel more comfortable reproducing our work. Please feel free to contact me with any questions or comments.\n","date":1603324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603324800,"objectID":"b9891a22c1410ab4a794ab1649bbea1e","permalink":"https://charlielehman.github.io/post/on-the-structures-of-representation/","publishdate":"2020-10-22T00:00:00Z","relpermalink":"/post/on-the-structures-of-representation/","section":"post","summary":"SS-08:Dynamic Background Reconstruction/Subtraction for Challenging Environments","tags":["Special Session","ICIP"],"title":"On the Structures of Representation for the Robustness of Semantic Segmentation to Input Corruption","type":"post"},{"authors":["Charlie Lehman","Dogancan Temel","Ghassan AlRegib"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"66694f0a8b2f9ce044166adcf35d3668","permalink":"https://charlielehman.github.io/publication/on-the-structures-of-representation/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/on-the-structures-of-representation/","section":"publication","summary":"Semantic segmentation is a scene understanding task at the heart of safety-critical applications where robustness to corrupted inputs is essential. Implicit Background Estimation (IBE) has demonstrated to be a promising technique to improve the robustness to out-of-distribution inputs for semantic segmentation models for little to no cost. In this paper, we provide analysis comparing the structures learned as a result of optimization objectives that use Softmax, IBE, and Sigmoid in order to improve understanding their relationship to robustness. As a result of this analysis, we propose combining Sigmoid with IBE (SCrIBE) to improve robustness. Finally, we demonstrate that SCrIBE exhibits superior segmentation performance aggregated across all corruptions and severity levels with a mIOU of 42.1 compared to both IBE 40.3 and the Softmax Baseline 37.5.","tags":["Machine Learning","Semantic Segmentation","Implicit Background Estimation"],"title":"On the Structures of Representation for the Robustness of Semantic Segmentation to Input Corruption","type":"publication"},{"authors":["Shirley Liu","Charlie Lehman","Ghassan AlRegib"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"72944d9334a5ddda6317f3b14278782f","permalink":"https://charlielehman.github.io/publication/robustness-and-overfitting/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/robustness-and-overfitting/","section":"publication","summary":"In this paper, we examine the overfitting behavior of image classification models modified with Implicit Background Estimation (SCrIBE), which transforms them into weakly supervised segmentation models that provide spatial domain visualizations without affecting performance. Using the segmentation masks, we derive an overfit detection criterion that does not require testing labels. In addition, we assess the change in model performance, calibration, and segmentation masks after applying data augmentations as overfitting reduction measures and testing on various types of distorted images.","tags":["Special Session","ICIP"],"title":"Robustness and Overfitting Behavior of Implicit Background Models","type":"publication"},{"authors":["Moamen Soliman","Charlie Lehman","Ghassan AlRegib"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"58f1d557656c3edf4e36c0c1b3553714","permalink":"https://charlielehman.github.io/publication/s6/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/s6/","section":"publication","summary":"Semi-supervised learning provides a means to leverage unlabeled data when labels are expensive to obtain. In this work, we propose a constrained framework that better learns from unlabeled data. The proposed algorithm adds an auxiliary task, image reconstruction, to the target segmentation task. The extra reconstruction task improves the model's geometric sense of different textures in an image. It happens that this improvement in geometrical reasoning of the model is transferable from reconstruction to segmentation since they both share a common part of the architecture. Such extra task allows the trained model to have richer representations and better geometric intuition. The presented results show that the proposed constrained framework achieves a boost in mean Intersection over Union by 18% than unconstrained one when using 2% of labeled examples. The boost in performance and reduction in amounts of labeled data is crucial for applications in which obtaining labels is expensive and labor intensive process such as Biomedical Imaging and Seismic Interpretation.","tags":["Machine Learning","Semantic Segmentation","ICIP"],"title":"S6:Semi-Supervised Self-Supervised Semantic Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"Introduction I have been faced with several situations where visualizing the output of a trained classifier has helped explain some interesting behaviors that are present within learned representations. I want to share a simple technique to visualize outputs for SoftMax-based classifiers. As an example, I will walk through the process of visualizing and animating the effects of Temperature Scaling, which is a simple and useful technique for knowledge transfer [1] and model calibration [2][3][4].\nSoftmax For deep neural networks (DNN) the representation is related to the construction of the optimization objective. In the case of DNN image classifiers the most common objective is to minimize the softmax cross entropy between the model output, \\(\\boldsymbol{v}\\in\\mathbb{R}^k\\) and a one-hot target, $\\boldsymbol{y}$. In order to compute the cross entropy, \\(\\boldsymbol{v}\\) must first be projected onto a simplex to become \u0026quot;probability-like\u0026quot;. \\( \\boldsymbol{\\sigma}:\\mathbb{R}^k\\rightarrow\\boldsymbol{\\Delta}^{k-1}\\\\ \\) The resulting vector, $\\boldsymbol{q}\\in\\boldsymbol{\\Delta}^{k-1}$, is the output of the softmax operation, $\\boldsymbol{\\sigma}$. To simplify notation, let \\(\\mathbf{e}^{\\boldsymbol{v}}= \\left(\\begin{smallmatrix}e^{v_0}\u0026 e^{v_1}\u0026\\dots\u0026e^{v_{k-1}}\\end{smallmatrix}\\right)\\).\n\\[ \\boldsymbol{q} = \\frac{\\mathbf{e}^{\\boldsymbol{v}}}{\\sum^{k-1}_{i=0} e^{v_i}} \\]\nHere's a visualization of SoftMax for the $k=2$ case.\n\nIt is then possible to assign a confidence, $c$, to a prediction by selecting the maximum component $c = \\max(\\boldsymbol{q})$.\n\nVisualizing Softmax at higher dimensions My research mostly concerns classification or detection problems for images, which tends to involve more than 2 classes. In order to visualize the behavior of softmax at $k\u0026gt;2$ we want to stay away from techniques that rely on the data directly (t-SNE, UMAP, PCA, etc.) and instead use a technique that can capture the macro behaviors in the representation space by prior construction. Doing this is likely not without loss of information, because data-driven methods seek to find some transformation that preserves information.\nThe first thing to do is to inspect the space to which softmax projects $\\boldsymbol v$, the $(k-1)$-simplex $\\boldsymbol{\\Delta}^{k-1}$, to better understand some useful properties for projection. Loosely defined, a simplex is the generalization of the triangle. In the case of the triangle it would be a 2-simplex. Below I generated a tikz visualization of the 0 to 3 simplexes:\n\nTake note of the graphical structure of simplexes as it will come into play in later discussion. Now for a more formal definition of a simplex with regard to our softmax projection, \\(\\boldsymbol{\\sigma}:\\mathbb{R}^k\\rightarrow\\boldsymbol{\\Delta}^{k-1}\\).\nAs we previously defined, $\\mathbf{q}$ is the resulting projected vector that follows the definition,\n\\[ \\{\\mathbf{q}\\in\\mathbb{R}^k:\\sum_i q_i=1, q_i\\geq 0, i=0,\\dots,k-1\\}, \\]\nwhich allows it to be used as a probability distribution. The fact that all of the information is projected into the positive orthant is also useful as now every component, $q_i\\in\\mathbf{q}$, can be used to form a star-like graph about the origin:\n\nThis projection is just a way to view these components in such a way at arbitrary dimensions such that the angle between each when projected is equal. And thus, forevermore throughout this post, such a technique shall be called the Equiradial Projection (EqR).\nWhere the angle between components is\n\\[ \\theta_i = \\frac{2\\pi (i+0.5)}{k}, \\quad i=\\{0,\\dots,k-1\\} \\]\nand the projection matrix is\n\\[ \\boldsymbol{T} = \\begin{pmatrix} \\sin(\\theta_0) \u0026\\sin(\\theta_1) \u0026\\cdots \u0026 \\sin(\\theta_{k-1}) \\\\ \\cos(\\theta_0) \u0026\\cos(\\theta_1) \u0026\\cdots \u0026 \\cos(\\theta_{k-1}) \\\\ \\end{pmatrix},\\\\ \\]\nmaking $\\boldsymbol{T}\\boldsymbol{q}$ just the weighted average of the projected components. This is where distortions and loss of information come into play. The derivation of the projection is based on the assumption that the space is star-like, meaning that each component occurs in isolation. We know that this is in fact not the case most of the time as $\\boldsymbol{q}$ by definition is a probability distribution making all but $k$ cases violate this assumption.\nNow that this loss of information and distortion is acknowledged we need to discuss rotation. When the order of the components changes in the projection the view is just rotating about another component orthoganally. This can provide some limited ability to minimize distortion by placing the most dependent subsets of components adjacent.\nTemperature Scaling Hinton et al. [1] introduced technique used for knowledge transfer called \u0026quot;Knowledge Distillation\u0026quot; that increases the temperature, $T$, on $\\boldsymbol{v}$ in order to generate a softer representation out of softmax. In [2], Guo et al, refers to the same technique as \u0026quot;Temperature Scaling\u0026quot;:\n\\[ \\boldsymbol{q}_{temp} = \\frac{\\mathbf{e}^{\\boldsymbol{v}/T}}{\\sum^{k-1}_{i=0} e^{v_i/T}} , \\qquad T\\in\\mathbb{R}^+ \\]\nThis simple technique has proven to be incredibly useful beyond the initial proposed use in knowledge transfer. Guo et al. demonstrated its utility in calibrating deep models for image classification. In both [3][4], they use temperature scaling to great effect in detecting out-of-distribution samples.\nAnd without further ado, let's visualize temperature scaled softmax outputs in arbitrary dimensions!\nFor $\\boldsymbol{v}$, I just generated random vectors in the desired dimension and visualized the projection at a variation of $T$. Each vector is colored according to the confidence at that location on the projection.\n\nCitations 1: G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” ArXiv, vol. abs/1503.02531, 2015.\n2: C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in Proceedings of the 34th international conference on machine learning-volume 70, 2017, pp. 1321–1330.\n3: K. Lee, H. Lee, K. Lee, and J. Shin, “Training confidence-calibrated classifiers for detecting out-of-distribution samples,” in International conference on learning representations, 2018.\n4: S. Liang, Y. Li, and R. Srikant, “Enhancing the reliability of out-of-distribution image detection in neural networks,” in International conference on learning representations, 2018.\n","date":1575849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575849600,"objectID":"d3d6692b90d13dc004bcce8bf3bb44da","permalink":"https://charlielehman.github.io/post/visualizing-tempscaling/","publishdate":"2019-12-09T00:00:00Z","relpermalink":"/post/visualizing-tempscaling/","section":"post","summary":"Introduction I have been faced with several situations where visualizing the output of a trained classifier has helped explain some interesting behaviors that are present within learned representations. I want to share a simple technique to visualize outputs for SoftMax-based classifiers.","tags":null,"title":"Visualizing Softmax","type":"post"},{"authors":null,"categories":null,"content":"Import Important Things import torch from torch import nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import transforms from torchvision.models import resnet18 from torchvision.datasets import CIFAR10 from tqdm import tqdm_notebook as tqdm from torchvision.utils import save_image, make_grid from matplotlib import pyplot as plt from matplotlib.colors import hsv_to_rgb from matplotlib.image import BboxImage from matplotlib.transforms import Bbox, TransformedBbox import numpy as np from IPython import display import requests from io import BytesIO from PIL import Image from PIL import Image, ImageSequence from IPython.display import HTML import warnings from matplotlib import rc import gc import matplotlib matplotlib.rcParams['pdf.fonttype'] = 42 matplotlib.rcParams['ps.fonttype'] = 42 gc.enable() plt.ioff()  Initialize the tiny model from ResNet18 I am replacing the first 7x7 conv stride of 4 with a 3x3 convolution kernel with stride of 1 and replacing maxpool with upsample. This keeps the spatial features from being downsampled too quickly as the forward pass propagates. The linear layer is replaced with a \u0026quot;pixelwise linear layer\u0026quot;, or a 1x1 convolution with stride of 1. This can be simply thought of as projecting a 1x512 vector (pixel) with a 512x10 matrix (1x1 conv). Notice that there is no operation that performs a spatial aggregation so what we have left is a 10x32x32 tensor after the final upsample. This can be used the same way as a semantic segmentation output, which we can also aggregate spatially and optimize using image level labels.\nnum_classes = 10 resnet = resnet18(pretrained=True) resnet.conv1 = nn.Conv2d(3,64,3,stride=1,padding=1) resnet_ = list(resnet.children())[:-2] resnet_[3] = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) classifier = nn.Conv2d(512,num_classes,1) torch.nn.init.kaiming_normal_(classifier.weight) resnet_.append(classifier) resnet_.append(nn.Upsample(size=32, mode='bilinear', align_corners=False)) tiny_resnet = nn.Sequential(*resnet_)  Define Attention In short, I'm going to just define 0 as the threshold in the logit (pre-softmax space). By selecting the largest component of the logit vector and then running it through sigmoid we can get a value with a support from 0 to 1, which is useful for inspecting the \u0026quot;attention\u0026quot; of the model.\ndef attention(x): return torch.sigmoid(torch.logsumexp(x,1, keepdim=True))  CIFAR10 dataset This dataset is so convenient for demonstrating so many things. There are much more impressive demonstrations of weak segmentation, but all of this can be accomplished in a jupyter notebook so here we go!\ntransform_train = transforms.Compose([ transforms.RandomCrop(32, padding=8), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ]) transform_test = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ]) trainset = CIFAR10(root='.', train=True, download=True, transform=transform_train) train_iter = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True, drop_last=True) testset = CIFAR10(root='.', train=False, download=True, transform=transform_test) test_iter = DataLoader(testset, batch_size=100, shuffle=False, num_workers=16, pin_memory=True) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')  Files already downloaded and verified Files already downloaded and verified  Train and Visualize The key take aways from the below code is that the objective for optimization is Binary Cross Entropy and the model's spatial aggregation is accomplished with a smooth-max operation. This means after aggregation the vector is optimized to be a set of 10 binary detectors, which is in contrast to the most popular method of characterization: softmax cross entropy, which encourages each pixel to select only one. When combined with the aforementioned attention operation we can forego aggregation and directly inspect exactly what the model uses to make a decision!\nmodel = nn.DataParallel(tiny_resnet).cuda() num_epochs = 10 criterion = nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9, weight_decay=1e-4) lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,78,eta_min=0.001) losses = [] acces = [] v_losses = [] v_acces = [] for epoch in tqdm(range(num_epochs)): epoch_loss = 0.0 acc = 0.0 var = 0.0 model.train() train_pbar = train_iter for i, (x, _label) in enumerate(train_pbar): x = x.cuda() _label = _label.cuda() label = F.one_hot(_label).float() seg_out = model(x) attn = attention(seg_out) # Smooth Max Aggregation logit = torch.log(torch.exp(seg_out*0.5).mean((-2,-1)))*2 loss = criterion(logit, label) optimizer.zero_grad() loss.backward() optimizer.step() lr_scheduler.step() epoch_loss += loss.item() acc += (logit.argmax(-1)==_label).sum() #train_pbar.set_description('Accuracy: {:.3f}%'.format(100*(logit.argmax(-1)==_label).float().mean())) avg_loss = epoch_loss / (i + 1) losses.append(avg_loss) avg_acc = acc.cpu().detach().numpy() / (len(trainset)) acces.append(avg_acc) model.eval() epoch_loss = 0.0 acc = 0.0 num_seen = 0 test_pbar = tqdm(test_iter) for i, (x, _label) in enumerate(test_pbar): x = x.cuda() _label = _label.cuda() label = F.one_hot(_label).float() seg_out = model(x) attn = attention(seg_out) logit = torch.log(torch.exp(seg_out*0.5).mean((-2,-1)))*2 loss = criterion(logit, label) epoch_loss += loss.item() acc += (logit.argmax(-1)==_label).sum() num_seen += label.size(0) test_pbar.set_description('Accuracy: {:.3f}%'.format(100*(acc.float()/num_seen))) avg_loss_val = epoch_loss / (i + 1) v_losses.append(avg_loss_val) avg_acc_val = acc.cpu().detach().numpy() / (len(testset)) v_acces.append(avg_acc_val) plt.close('all') conf = torch.max(nn.functional.softmax(seg_out, dim=1), dim=1)[0] hue = (torch.argmax(seg_out, dim=1).float() + 0.5)/10 x -= x.min() x /= x.max() gs_im = x.mean(1) gs_mean = gs_im.mean() gs_min = gs_im.min() gs_max = torch.max((gs_im-gs_min)) gs_im = (gs_im - gs_min)/gs_max hsv_im = torch.stack((hue.float(), attn.squeeze().float(), gs_im.float()), -1) im = hsv_to_rgb(hsv_im.cpu().detach().numpy()) ex = make_grid(torch.tensor(im).permute(0,3,1,2), normalize=True, nrow=25) attns = make_grid(attn, normalize=False, nrow=25) attns = attns.cpu().detach() inputs = make_grid(x, normalize=True, nrow=25).cpu().detach() display.clear_output(wait=True) plt.figure(figsize=(20,8)) plt.imshow(np.concatenate((inputs.numpy().transpose(1,2,0),ex.numpy().transpose(1,2,0), attns.numpy().transpose(1,2,0)), axis=0)) #plt.xticks(np.linspace(18,324,10), classes) #plt.xticks(fontsize=20) plt.yticks([]) plt.title('CIFAR10 Epoch:{:02d}, Train:{:.3f}, Test:{:.3f}'.format(epoch, avg_acc, avg_acc_val), fontsize=20) display.display(plt.gcf()) fig, ax = plt.subplots(1,2, figsize=(20,8)) ax[0].set_title('Crossentropy') ax[0].plot(losses, label='Train') ax[0].plot(v_losses, label='CIFAR10 Test') ax[0].legend() ax[1].set_title('Accuracy') ax[1].plot(acces, label='Train') ax[1].plot(v_acces, label='CIFAR10 Test') ax[1].legend() display.display(plt.gcf())  I only trained it for 10 epochs here and get a passable performance, which does improve if it goes further. I stopped it to leave some of the mixed decisions the model is making. Declaring success here is premature for calling this as a great method for weak segmentation, but it does show exactly what the model considers spatially for every decision it makes. Once more, the model only uses values that are very positive thus saturating sigmoid to make a decision, by combining the argmax with the attention operation defined we can get the below visualization. The examples that show multiple colors are examples that share features with other classes, i.e. the birds and airplanes, deer and horses, cars and trucks. Now if there were only a method for looking at where all the pixels project into the learned space all at once!\n\nThough not absolutely necessary here's what the numbers look like during training. \n","date":1571270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571270400,"objectID":"e1cbf4aea1660dc89f0259975f1b8877","permalink":"https://charlielehman.github.io/post/weak-segmentation-cifar10/","publishdate":"2019-10-17T00:00:00Z","relpermalink":"/post/weak-segmentation-cifar10/","section":"post","summary":"Import Important Things import torch from torch import nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import transforms from torchvision.models import resnet18 from torchvision.datasets import CIFAR10 from tqdm import tqdm_notebook as tqdm from torchvision.","tags":null,"title":"Learning to segment CIFAR10","type":"post"},{"authors":["Charlie Lehman","Dogancan Temel","Ghassan AlRegib"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"b8401e4cb6180ab963cde58ab9c1edb0","permalink":"https://charlielehman.github.io/publication/implicit-background-estimation-for-semantic-segmentation/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/implicit-background-estimation-for-semantic-segmentation/","section":"publication","summary":"Scene understanding and semantic segmentation are at the core of many computer vision tasks, many of which, involve interacting with humans in potentially dangerous ways. It is therefore paramount that techniques for principled design of robust models be developed. In this paper, we provide analytic and empirical evidence that correcting potentially errant non-distinct mappings that result from the softmax function can result in improving robustness characteristics on a state-of-the-art semantic segmentation model with minimal impact to performance and minimal changes to the code base.","tags":["ICIP","Machine Learning","Semantic Segmentation","Implicit Background Estimation"],"title":"Implicit Background Estimation for Semantic Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"I was interviewed by Julie Lindsay from Speechworks for a podcast regarding my experiences starting ConvexMind during the CreateX Startup Launch last Summer. There is also an article that summarizes the main points we discussed.\n","date":1565654400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565654400,"objectID":"ba87cce6b9ea62cd34c218cc45a13a8b","permalink":"https://charlielehman.github.io/post/speechworks-interview/","publishdate":"2019-08-13T00:00:00Z","relpermalink":"/post/speechworks-interview/","section":"post","summary":"I was interviewed by Julie Lindsay from Speechworks for a podcast regarding my experiences starting ConvexMind during the CreateX Startup Launch last Summer. There is also an article that summarizes the main points we discussed.","tags":null,"title":"Speechworks Interview","type":"post"},{"authors":null,"categories":null,"content":"I worked with Gukyeong Kwon and Jinsol Lee on this project for our Convex Optimization course. It was a neat project that really hits home that even if you can count cards perfectly\u0026hellip;the deck isn\u0026rsquo;t stacked in your favor. If you want to try it the code is linked above or if you want to run blackjacksim directly install it with:\npip3 install git+https://github.com/charlieLehman/blackjacksim  Tools from sklearn.metrics import confusion_matrix from sklearn.utils.multiclass import unique_labels import matplotlib from mpl_toolkits.axes_grid1 import make_axes_locatable %matplotlib inline from matplotlib import pyplot as plt def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues, **kwargs): \u0026quot;\u0026quot;\u0026quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \u0026quot;\u0026quot;\u0026quot; if not title: if normalize: title = 'Normalized confusion matrix' else: title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix(y_true, y_pred) # Only use the labels that appear in the data #classes = classes[unique_labels(y_true, y_pred)] if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\u0026quot;Normalized confusion matrix\u0026quot;) else: print('Confusion matrix, without normalization') print(cm) fig, ax = plt.subplots(**kwargs) im = ax.imshow(cm, interpolation='nearest', cmap=cmap) divider = make_axes_locatable(ax) cax = divider.append_axes(\u0026quot;right\u0026quot;, size=\u0026quot;5%\u0026quot;, pad=0.05) ax.figure.colorbar(im, cax=cax) # We want to show all ticks... ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), # ... and label them with the respective list entries xticklabels=classes, yticklabels=classes, ylabel='True label', xlabel='Predicted label') # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=45, ha=\u0026quot;right\u0026quot;, rotation_mode=\u0026quot;anchor\u0026quot;) # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(j, i, format(cm[i, j], fmt), ha=\u0026quot;center\u0026quot;, va=\u0026quot;center\u0026quot;, color=\u0026quot;white\u0026quot; if cm[i, j] \u0026gt; thresh else \u0026quot;black\u0026quot;) fig.tight_layout() return ax  Comparison of House Rules from blackjacksim.simulations import Game from blackjacksim.entities import Shoe import matplotlib %matplotlib inline from tqdm import tnrange from tqdm import tqdm_notebook as tqdm from matplotlib import pyplot as plt import seaborn as sns import pandas as pd from jupyterthemes import jtplot jtplot.style(context='poster', fscale=1.4, spines=False, gridlines='--') from blackjacksim.data import DefaultGameConfig _def_conf = DefaultGameConfig() def config(house_rules): _def_conf['house']['class'] = house_rules return _def_conf try: df = df except: df = None pbar = tqdm(['Blackjack32', 'Blackjack65', 'Blackjack32NoSplit', 'Blackjack65NoSplit']) trials = 100 rounds = 100 for house in pbar: for i in range(trials): pbar.set_description(\u0026quot;{} {:04d}/{:04d}: \u0026quot;.format(house,i,trials-1)) g = Game(config(house)) for _ in range(rounds): g.round() if df is None: df = g.data else: df = pd.concat([df,g.data]) sns.lineplot(x='Round', y='Pool', hue='House', data=df) plt.show()  HBox(children=(IntProgress(value=0, max=4), HTML(value='')))  Modeling Action Strategy Build board state matrix A and action vector b from blackjacksim.entities import Deck, Hand from blackjacksim.strategies import basic import itertools import numpy as np import pandas as pd action_to_class = {'Hit':[1,0,0,0],'Stand':[0,1,0,0],'Split':[0,0,1,0],'Double':[0.0,0,0,1]} hands = [Hand(h) for h in itertools.product(Deck(),Deck())] dups = Deck() t = [] for hand, dup in itertools.product(hands, dups): tup = tuple(c.value for c in (*hand,dup)) c = (tup, basic(hand,dup)) if c not in t: t.append(c) print(len(t)) A = [] b = [] for a, _b in t: A.append(a) b.append(action_to_class[_b]) A = np.stack(A) b = np.array(b)  1001  Solve Least Squares import seaborn as sns from jupyterthemes import jtplot jtplot.style(context='paper', fscale=1.4, spines=False, gridlines='') A_ = np.concatenate([A, np.ones((A.shape[0],1))],1) Ai = np.linalg.pinv(A_) x = Ai@b out = A_@x pred = np.argmax(out,1) lab = np.argmax(b,1) lab_to_class = list(action_to_class.keys()) l2c = lambda x: lab_to_class[x] df = pd.DataFrame({'Prediction':pred, 'Label':lab,'HandSum':A[:,0:-1].sum(1), 'Hand':[a[0:-1] for a in A], 'Up Card':[a[-1] for a in A]}) df['Label Name'] = df.Label.apply(l2c) df['Prediction Name'] = df.Prediction.apply(l2c) df['Correct'] = df.Prediction == df.Label # Plot normalized confusion matrix classes = list(action_to_class.keys()) print('Accuracy: {:.2f}%\\n'.format(df.Correct.mean()*100)) plot_confusion_matrix(lab, pred, classes=classes, normalize=True, title=' ', figsize=(6,6)) plt.show()  Accuracy: 65.53% Normalized confusion matrix [[0.82826087 0.14782609 0. 0.02391304] [0.11551155 0.88448845 0. 0. ] [0.45833333 0.54166667 0. 0. ] [0.7 0.26315789 0. 0.03684211]]  Solve SVM with RBF kernel from sklearn.svm import SVC clf = SVC(gamma='auto', probability=True) label = b.argmax(1) clf.fit(A,label) print(clf.score(A,label)) pred = clf.predict(A) vals = clf.decision_function(A) probs = clf.predict_proba(A) classes = list(action_to_class.keys()) plot_confusion_matrix(label, pred, classes=classes, normalize=True, title=' ', figsize=(6,6)) plt.show()  0.932067932067932 Normalized confusion matrix [[0.9826087 0.00869565 0. 0.00869565] [0.00660066 0.98679868 0. 0.00660066] [0.41666667 0.45833333 0.10416667 0.02083333] [0.04736842 0.02105263 0. 0.93157895]]  Comparison of Optimizers for a Deep Model Train from blackjacksim.entities import Deck, Hand from blackjacksim.strategies import basic import itertools import numpy as np import pandas as pd import torch from torch import nn from tqdm import tnrange # Build A (Hand and Dealer's Up Card) and b (basic strategy Action) action_to_class = {'Hit':[1,0,0,0],'Stand':[0,1,0,0],'Split':[0,0,1,0],'Double':[0.0,0,0,1]} hands = [Hand(h) for h in itertools.product(Deck(),Deck())] dups = Deck() t = [] for hand, dup in itertools.product(hands, dups): tup = tuple(c.value for c in (*hand,dup)) c = (tup, basic(hand,dup)) if c not in t: t.append(c) print(len(t)) A = [] b = [] for a, _b in t: A.append(a) b.append(action_to_class[_b]) A = np.stack(A) b = np.array(b) A = torch.from_numpy(A).float() b = torch.from_numpy(b).float() # Build Deep Model class DeepBasicStrategy(nn.Module): def __init__(self): super(DeepBasicStrategy, self).__init__() block = lambda i, o: nn.Sequential( nn.Linear(i,o), nn.BatchNorm1d(o), nn.ReLU(), nn.Dropout(), ) _model = [] for i,o in [(3,2000), (2000,2000), (2000,1000), (1000,500), (500,250)]: _model.append(block(i,o)) _model.append(nn.Linear(250,4)) self.neural_net = nn.Sequential(*_model) def forward(self, x): return self.neural_net(x) A = A.cuda() b = b.cuda() # Train Deep Model criterion = nn.BCEWithLogitsLoss() train_log = [] for _ in tnrange(1, position=0): for opt_name in ['SGD', 'SGD w/ momentum', 'SGD w/ Nesterov momentum', 'Adam']: model = DeepBasicStrategy() model = model.cuda() closure = None if opt_name == 'SGD': optimizer = torch.optim.SGD(model.parameters(), lr=1.0) elif opt_name == 'SGD w/ momentum': optimizer = torch.optim.SGD(model.parameters(), lr=1.0, momentum=0.9) elif opt_name == 'SGD w/ Nesterov momentum': optimizer = torch.optim.SGD(model.parameters(), lr=1.0, momentum=0.9, nesterov=True) elif opt_name == 'Adam': optimizer = torch.optim.Adam(model.parameters()) elif opt_name == 'LBFGS': optimizer = torch.optim.LBFGS(model.parameters(), lr=1.0)#, history_size=100, max_iter=3, max_eval=4) closure = lambda: criterion(model(A),b) tbar = tnrange(1000, position=1) for step in tbar: optimizer.zero_grad() model.train() out = model(A) loss = criterion(out, b) model.eval() out = model(A) pred = out.argmax(1) label = b.argmax(1) acc = (pred==label).float().mean().item() tbar.set_description(\u0026quot;BCE Loss: {:.3f} Acc: {:.3f}\u0026quot;.format(loss.item(), acc)) loss.backward() optimizer.step(closure) train_log.append( { 'Optimizer':opt_name, 'Step':step, 'Accuracy':acc, 'Loss':loss.item(), } )  1001 HBox(children=(IntProgress(value=0, max=1), HTML(value=''))) HBox(children=(IntProgress(value=0, max=1000), HTML(value=''))) HBox(children=(IntProgress(value=0, max=1000), HTML(value=''))) HBox(children=(IntProgress(value=0, max=1000), HTML(value=''))) HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))  Plot loss and accuracy import matplotlib %matplotlib inline from matplotlib import pyplot as plt import seaborn as sns from jupyterthemes import jtplot train_df = pd.DataFrame(train_log) train_df['Error'] = 1-train_df.Accuracy # set \u0026quot;context\u0026quot; (paper, notebook, talk, poster) # scale font-size of ticklabels, legend, etc. # remove spines from x and y axes and make grid dashed jtplot.style(context='paper', fscale=1.4, spines=False, gridlines='--') fig,ax = plt.subplots(1, figsize=(7, 5)) ax.set(yscale='log') sns.lineplot(x='Step', y='Loss', hue='Optimizer', data=train_df, ax=ax) plt.show() fig,ax = plt.subplots(1, figsize=(7, 5)) ax.set(ylim=[.6,1.05]) sns.lineplot(x='Step', y='Accuracy', hue='Optimizer', data=train_df, ax=ax) plt.show() jtplot.style(context='paper', fscale=1.4, spines=False, gridlines='') classes = list(action_to_class.keys()) plot_confusion_matrix(label.cpu(), pred.cpu(), classes=classes, normalize=True, title=' ', figsize=(6,6)) plt.show()  Normalized confusion matrix [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]]  Visualization of A, RBF, and Deep Model representations from sklearn.decomposition import PCA a = A.cpu().detach().numpy() pca = PCA(2) y = pca.fit_transform(a) l = label.cpu().detach().numpy() p = pred.cpu().detach().numpy() X = out.cpu().detach().numpy() fig, ax = plt.subplots(1,3, figsize=(15,5)) ax[0].scatter(y[l==0,0], y[l==0,1], label='Hit') ax[0].scatter(y[l==1,0], y[l==1,1], label='Stand') ax[0].scatter(y[l==2,0], y[l==2,1], label='Split') ax[0].scatter(y[l==3,0], y[l==3,1], label='Double') ax[0].set_xticks([]) ax[0].set_yticks([]) ax[0].set_title('Raw') pca = PCA(2) y = pca.fit_transform(X) ax[2].scatter(y[l==0,0], y[l==0,1], label='Hit') ax[2].scatter(y[l==1,0], y[l==1,1], label='Stand') ax[2].scatter(y[l==2,0], y[l==2,1], label='Split') ax[2].scatter(y[l==3,0], y[l==3,1], label='Double') ax[2].set_xticks([]) ax[2].set_yticks([]) ax[2].set_title('Deep Model') pca = PCA(2) y = pca.fit_transform(vals) hit = ax[1].scatter(y[l==0,0], y[l==0,1], label='Hit') stand = ax[1].scatter(y[l==1,0], y[l==1,1], label='Stand') split = ax[1].scatter(y[l==2,0], y[l==2,1], label='Split') double = ax[1].scatter(y[l==3,0], y[l==3,1], label='Double') ax[1].set_xticks([]) ax[1].set_yticks([]) ax[1].set_title('RBF Kernel') fig.legend(['Hit','Stand','Split','Double'], bbox_to_anchor=[0.39, 0.05], loc='center', ncol=4) plt.show()  ","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"f57e1a9362c22a7ebc3730f8beef5842","permalink":"https://charlielehman.github.io/project/card-counting/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/project/card-counting/","section":"project","summary":"Project for ECE8823 Convex Optimization course at Georgia Tech","tags":["Machine Learning","Class Project"],"title":"Blackjack Simulator","type":"project"},{"authors":null,"categories":null,"content":"Pixelwise classifiers with Large-margin Gaussian Mixture representation These are several visualizations I dug up from really really messy code, which I won't share just yet. This was some quick and dirty work to test out the feasibility of an idea: I wanted to visualize the effects on the representation space at each pixel during an FGSM attack. I trained a weak segmentor model with the representation space modified for L-GM. I found L-GM useful because it provides a sensible way of directly learning a representation that can be visualized without further solving an additional optimization problem (looking at you T-SNE!). Not to mention you can project new points into it, which was really central to my purpose of observing the effects of an adversarial attack. Original Truck Image This is what a normal input from the CIFAR10 test set looks like.\nThe max-softmax, which is a good baseline for confidence in detection tasks drops from 1.00 to 0.72 after adversarial modification. I also computed a confidence term that looked for regions of \u0026quot;uncertainty\u0026quot;, which I am using loosely. Specifically I was interested in regions close to 0.5 when using sigmoid on the raw logits and the logits post temperature scaling. I saw that when the model is behaving well there is a uniform expansion of the uncertain regions spatially resulting in little overlap between the raw and temperature scaled regions. So by multiplying both taking the spatial average I can determine the overall uncertainy and by additive inverse get a confidence measure for observing adversarial images spatially. Original Truck Image This is what an untargeted FGSM attack looks like.\nIn this case it appears that FGSM just messed with the image's high contrast regions on the cab of the truck. My confidence measure drops to 0.64 from 0.92, but the detection still clears the 0 threshold sigmoid learned.\nWhen CLAHE is applied to both images we can see some interesting effects.\nOriginal Truck Image\nOn the original image, my confidence measure drops from 0.92 to 0.84 and the pixel embedding spreads out far more.\nOriginal Truck Image\nOn the FGSM image, my confidence measure plummets from 0.64 to 0.21. Interestingly, the pixel embedding stretches back toward truck.\n","date":1544659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544659200,"objectID":"77db529b52675b73adf672146bfc83a7","permalink":"https://charlielehman.github.io/post/visualizing-fgsm/","publishdate":"2018-12-13T00:00:00Z","relpermalink":"/post/visualizing-fgsm/","section":"post","summary":"Pixelwise classifiers with Large-margin Gaussian Mixture representation These are several visualizations I dug up from really really messy code, which I won't share just yet. This was some quick and dirty work to test out the feasibility of an idea: I wanted to visualize the effects on the representation space at each pixel during an FGSM attack.","tags":null,"title":"Visualizing Adversarial Attacks","type":"post"},{"authors":null,"categories":null,"content":"I spent some time going through Tensorflow tutorials and was curious what the learned kernels in the convolution layers looked like. After getting a visulization to work for regular RGB input images from CIFAR10, I transformed and created copies of the dataset using other representations: FFT, DCT, and HSV. The same model used to train the RGB inputs was used and similar performance was achieved.\nIf you would like to learn more you can implement it yourself here\n","date":1485820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485820800,"objectID":"2e69ff0d5ec35715a1a4034f3e6b8cf8","permalink":"https://charlielehman.github.io/post/multi-basis-input/","publishdate":"2017-01-31T00:00:00Z","relpermalink":"/post/multi-basis-input/","section":"post","summary":"I spent some time going through Tensorflow tutorials and was curious what the learned kernels in the convolution layers looked like. After getting a visulization to work for regular RGB input images from CIFAR10, I transformed and created copies of the dataset using other representations: FFT, DCT, and HSV.","tags":null,"title":"Multi-Basis Input Convolutional Neural Network","type":"post"},{"authors":["Charlie Lehman","Robert J. Barsanti"],"categories":null,"content":"","date":1204329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1204329600,"objectID":"4bf32232fcb6d91745426bad822c1fc9","permalink":"https://charlielehman.github.io/publication/application-of-a-wavelet-based-receiver-for-the-coherent-detection-of-fsk-signals/","publishdate":"2008-03-01T00:00:00Z","relpermalink":"/publication/application-of-a-wavelet-based-receiver-for-the-coherent-detection-of-fsk-signals/","section":"publication","summary":"This paper investigates the application of wavelet analysis to the problem of coherent detection of digital binary frequency shift keying communication signals in additive white Gaussian noise channels. The proposed wavelet-based receiver computes the normalized cross correlation between the filtered wavelet coefficients of the received signal and wavelet coefficients that correspond to the known transmitted FSK signals. Simulations are conducted comparing the wavelet receiver to the classical coherent FSK receiver.","tags":["Digital Signal Processing"],"title":"Application of a Wavelet-Based Receiver for the Coherent Detection of FSK Signals","type":"publication"}]