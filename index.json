[{"authors":["admin"],"categories":null,"content":"Charlie Lehman is a Machine Learning Ph.D. student at the Georgia Tech Omni Lab fro Visual Engineering and Science (OLIVES). His research interests include robustness and explainability of deep vision models. He is also an Engineering Duty Officer with the U.S. Navy Reserves, where he specializes in the maintenance and repair of surface vessels.\n","date":1567296000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567296000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://charlielehman.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Charlie Lehman is a Machine Learning Ph.D. student at the Georgia Tech Omni Lab fro Visual Engineering and Science (OLIVES). His research interests include robustness and explainability of deep vision models. He is also an Engineering Duty Officer with the U.S. Navy Reserves, where he specializes in the maintenance and repair of surface vessels.","tags":null,"title":"Charlie Lehman","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://charlielehman.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://charlielehman.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://charlielehman.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://charlielehman.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Introduction I have been faced with several situations where visualizing the output of a trained classifier has helped explain some interesting behaviors that are present within learned representations. I want to share a simple technique to visualize outputs for SoftMax-based classifiers. As an example, I will walk through the process of visualizing and animating the effects of Temperature Scaling, which is a simple and useful technique for knowledge transfer 1 and model calibration 234.\nSoftmax For deep neural networks (DNN) the representation is related to the construction of the optimization objective. In the case of DNN image classifiers the most common objective is to minimize the softmax cross entropy between the model output, \\(\\boldsymbol{v}\\in\\mathbb{R}^k\\) and a one-hot target, $\\boldsymbol{y}$. In order to compute the cross entropy, \\(\\boldsymbol{v}\\) must first be projected onto a simplex to become \u0026quot;probability-like\u0026quot;. \\( \\boldsymbol{\\sigma}:\\mathbb{R}^k\\rightarrow\\boldsymbol{\\Delta}^{k-1}\\\\ \\) The resulting vector, $\\boldsymbol{q}\\in\\boldsymbol{\\Delta}^{k-1}$, is the output of the softmax operation, $\\boldsymbol{\\sigma}$. To simplify notation, let \\(\\mathbf{e}^{\\boldsymbol{v}}= \\left(\\begin{smallmatrix}e^{v_0}\u0026 e^{v_1}\u0026\\dots\u0026e^{v_{k-1}}\\end{smallmatrix}\\right)\\).\n\\[ \\boldsymbol{q} = \\frac{\\mathbf{e}^{\\boldsymbol{v}}}{\\sum^{k-1}_{i=0} e^{v_i}} \\]\nHere's a visualization of SoftMax for the $k=2$ case.\n \nIt is then possible to assign a confidence, $c$, to a prediction by selecting the maximum component $c = \\max(\\boldsymbol{q})$.\n\nVisualizing Softmax at higher dimensions My research mostly concerns classification or detection problems for images, which tends to involve more than 2 classes. In order to visualize the behavior of softmax at $k\u0026gt;2$ we want to stay away from techniques that rely on the data directly (t-SNE, UMAP, PCA, etc.) and instead use a technique that can capture the macro behaviors in the representation space by prior construction. Doing this is likely not without loss of information, because data-driven methods seek to find some transformation that preserves information.\nThe first thing to do is to inspect the space to which softmax projects $\\boldsymbol v$, the $(k-1)$-simplex $\\boldsymbol{\\Delta}^{k-1}$, to better understand some useful properties for projection. Loosely defined, a simplex is the generalization of the triangle. In the case of the triangle it would be a 2-simplex. Below I generated a tikz visualization of the 0 to 3 simplexes:\n\nTake note of the graphical structure of simplexes as it will come into play in later discussion. Now for a more formal definition of a simplex with regard to our softmax projection, \\(\\boldsymbol{\\sigma}:\\mathbb{R}^k\\rightarrow\\boldsymbol{\\Delta}^{k-1}\\).\nAs we previously defined, $\\mathbf{q}$ is the resulting projected vector that follows the definition,\n\\[ \\{\\mathbf{q}\\in\\mathbb{R}^k:\\sum_i q_i=1, q_i\\geq 0, i=0,\\dots,k-1\\}, \\]\nwhich allows it to be used as a probability distribution. The fact that all of the information is projected into the positive orthant is also useful as now every component, $q_i\\in\\mathbf{q}$, can be used to form a star-like graph about the origin:\n\nThis projection is just a way to view these components in such a way at arbitrary dimensions such that the angle between each when projected is equal. And thus, forevermore throughout this post, such a technique shall be called the Equiradial Projection (EqR).\nWhere the angle between components is\n\\[ \\theta_i = \\frac{2\\pi (i+0.5)}{k}, \\quad i=\\{0,\\dots,k-1\\} \\]\nand the projection matrix is\n\\[ \\boldsymbol{T} = \\begin{pmatrix} \\sin(\\theta_0) \u0026\\sin(\\theta_1) \u0026\\cdots \u0026 \\sin(\\theta_{k-1}) \\\\ \\cos(\\theta_0) \u0026\\cos(\\theta_1) \u0026\\cdots \u0026 \\cos(\\theta_{k-1}) \\\\ \\end{pmatrix},\\\\ \\]\nmaking $\\boldsymbol{T}\\boldsymbol{q}$ just the weighted average of the projected components. This is where distortions and loss of information come into play. The derivation of the projection is based on the assumption that the space is star-like, meaning that each component occurs in isolation. We know that this is in fact not the case most of the time as $\\boldsymbol{q}$ by definition is a probability distribution making all but $k$ cases violate this assumption.\nNow that this loss of information and distortion is acknowledged we need to discuss rotation. When the order of the components changes in the projection the view is just rotating about another component orthoganally. This can provide some limited ability to minimize distortion by placing the most dependent subsets of components adjacent.\nTemperature Scaling Hinton et al. 5 introduced technique used for knowledge transfer called \u0026quot;Knowledge Distillation\u0026quot; that increases the temperature, $T$, on $\\boldsymbol{v}$ in order to generate a softer representation out of softmax. In 6, Guo et al, refers to the same technique as \u0026quot;Temperature Scaling\u0026quot;:\n\\[ \\boldsymbol{q}_{temp} = \\frac{\\mathbf{e}^{\\boldsymbol{v}/T}}{\\sum^{k-1}_{i=0} e^{v_i/T}} , \\qquad T\\in\\mathbb{R}^+ \\]\nThis simple technique has proven to be incredibly useful beyond the initial proposed use in knowledge transfer. Guo et al. demonstrated its utility in calibrating deep models for image classification. In both 78, they use temperature scaling to great effect in detecting out-of-distribution samples.\nAnd without further ado, let's visualize temperature scaled softmax outputs in arbitrary dimensions!\nFor $\\boldsymbol{v}$, I just generated random vectors in the desired dimension and visualized the projection at a variation of $T$. Each vector is colored according to the confidence at that location on the projection.\n\nCitations   G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” ArXiv, vol. abs/1503.02531, 2015. ^ C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in Proceedings of the 34th international conference on machine learning-volume 70, 2017, pp. 1321–1330. ^ K. Lee, H. Lee, K. Lee, and J. Shin, “Training confidence-calibrated classifiers for detecting out-of-distribution samples,” in International conference on learning representations, 2018. ^ S. Liang, Y. Li, and R. Srikant, “Enhancing the reliability of out-of-distribution image detection in neural networks,” in International conference on learning representations, 2018. ^ G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” ArXiv, vol. abs/1503.02531, 2015. ^ C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in Proceedings of the 34th international conference on machine learning-volume 70, 2017, pp. 1321–1330. ^ K. Lee, H. Lee, K. Lee, and J. Shin, “Training confidence-calibrated classifiers for detecting out-of-distribution samples,” in International conference on learning representations, 2018. ^ S. Liang, Y. Li, and R. Srikant, “Enhancing the reliability of out-of-distribution image detection in neural networks,” in International conference on learning representations, 2018. ^   ","date":1575849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575849600,"objectID":"d3d6692b90d13dc004bcce8bf3bb44da","permalink":"https://charlielehman.github.io/post/visualizing-tempscaling/","publishdate":"2019-12-09T00:00:00Z","relpermalink":"/post/visualizing-tempscaling/","section":"post","summary":"Introduction I have been faced with several situations where visualizing the output of a trained classifier has helped explain some interesting behaviors that are present within learned representations. I want to share a simple technique to visualize outputs for SoftMax-based classifiers. As an example, I will walk through the process of visualizing and animating the effects of Temperature Scaling, which is a simple and useful technique for knowledge transfer 1 and model calibration 234.","tags":null,"title":"Visualizing Softmax","type":"post"},{"authors":null,"categories":null,"content":"Import Important Things import torch from torch import nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import transforms from torchvision.models import resnet18 from torchvision.datasets import CIFAR10 from tqdm import tqdm_notebook as tqdm from torchvision.utils import save_image, make_grid from matplotlib import pyplot as plt from matplotlib.colors import hsv_to_rgb from matplotlib.image import BboxImage from matplotlib.transforms import Bbox, TransformedBbox import numpy as np from IPython import display import requests from io import BytesIO from PIL import Image from PIL import Image, ImageSequence from IPython.display import HTML import warnings from matplotlib import rc import gc import matplotlib matplotlib.rcParams['pdf.fonttype'] = 42 matplotlib.rcParams['ps.fonttype'] = 42 gc.enable() plt.ioff()  Initialize the tiny model from ResNet18 I am replacing the first 7x7 conv stride of 4 with a 3x3 convolution kernel with stride of 1 and replacing maxpool with upsample. This keeps the spatial features from being downsampled too quickly as the forward pass propagates. The linear layer is replaced with a \u0026quot;pixelwise linear layer\u0026quot;, or a 1x1 convolution with stride of 1. This can be simply thought of as projecting a 1x512 vector (pixel) with a 512x10 matrix (1x1 conv). Notice that there is no operation that performs a spatial aggregation so what we have left is a 10x32x32 tensor after the final upsample. This can be used the same way as a semantic segmentation output, which we can also aggregate spatially and optimize using image level labels.\nnum_classes = 10 resnet = resnet18(pretrained=True) resnet.conv1 = nn.Conv2d(3,64,3,stride=1,padding=1) resnet_ = list(resnet.children())[:-2] resnet_[3] = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) classifier = nn.Conv2d(512,num_classes,1) torch.nn.init.kaiming_normal_(classifier.weight) resnet_.append(classifier) resnet_.append(nn.Upsample(size=32, mode='bilinear', align_corners=False)) tiny_resnet = nn.Sequential(*resnet_)  Define Attention In short, I'm going to just define 0 as the threshold in the logit (pre-softmax space). By selecting the largest component of the logit vector and then running it through sigmoid we can get a value with a support from 0 to 1, which is useful for inspecting the \u0026quot;attention\u0026quot; of the model.\ndef attention(x): return torch.sigmoid(torch.logsumexp(x,1, keepdim=True))  CIFAR10 dataset This dataset is so convenient for demonstrating so many things. There are much more impressive demonstrations of weak segmentation, but all of this can be accomplished in a jupyter notebook so here we go!\ntransform_train = transforms.Compose([ transforms.RandomCrop(32, padding=8), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ]) transform_test = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), ]) trainset = CIFAR10(root='.', train=True, download=True, transform=transform_train) train_iter = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True, drop_last=True) testset = CIFAR10(root='.', train=False, download=True, transform=transform_test) test_iter = DataLoader(testset, batch_size=100, shuffle=False, num_workers=16, pin_memory=True) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')  Files already downloaded and verified Files already downloaded and verified  Train and Visualize The key take aways from the below code is that the objective for optimization is Binary Cross Entropy and the model's spatial aggregation is accomplished with a smooth-max operation. This means after aggregation the vector is optimized to be a set of 10 binary detectors, which is in contrast to the most popular method of characterization: softmax cross entropy, which encourages each pixel to select only one. When combined with the aforementioned attention operation we can forego aggregation and directly inspect exactly what the model uses to make a decision!\nmodel = nn.DataParallel(tiny_resnet).cuda() num_epochs = 10 criterion = nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9, weight_decay=1e-4) lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,78,eta_min=0.001) losses = [] acces = [] v_losses = [] v_acces = [] for epoch in tqdm(range(num_epochs)): epoch_loss = 0.0 acc = 0.0 var = 0.0 model.train() train_pbar = train_iter for i, (x, _label) in enumerate(train_pbar): x = x.cuda() _label = _label.cuda() label = F.one_hot(_label).float() seg_out = model(x) attn = attention(seg_out) # Smooth Max Aggregation logit = torch.log(torch.exp(seg_out*0.5).mean((-2,-1)))*2 loss = criterion(logit, label) optimizer.zero_grad() loss.backward() optimizer.step() lr_scheduler.step() epoch_loss += loss.item() acc += (logit.argmax(-1)==_label).sum() #train_pbar.set_description('Accuracy: {:.3f}%'.format(100*(logit.argmax(-1)==_label).float().mean())) avg_loss = epoch_loss / (i + 1) losses.append(avg_loss) avg_acc = acc.cpu().detach().numpy() / (len(trainset)) acces.append(avg_acc) model.eval() epoch_loss = 0.0 acc = 0.0 num_seen = 0 test_pbar = tqdm(test_iter) for i, (x, _label) in enumerate(test_pbar): x = x.cuda() _label = _label.cuda() label = F.one_hot(_label).float() seg_out = model(x) attn = attention(seg_out) logit = torch.log(torch.exp(seg_out*0.5).mean((-2,-1)))*2 loss = criterion(logit, label) epoch_loss += loss.item() acc += (logit.argmax(-1)==_label).sum() num_seen += label.size(0) test_pbar.set_description('Accuracy: {:.3f}%'.format(100*(acc.float()/num_seen))) avg_loss_val = epoch_loss / (i + 1) v_losses.append(avg_loss_val) avg_acc_val = acc.cpu().detach().numpy() / (len(testset)) v_acces.append(avg_acc_val) plt.close('all') conf = torch.max(nn.functional.softmax(seg_out, dim=1), dim=1)[0] hue = (torch.argmax(seg_out, dim=1).float() + 0.5)/10 x -= x.min() x /= x.max() gs_im = x.mean(1) gs_mean = gs_im.mean() gs_min = gs_im.min() gs_max = torch.max((gs_im-gs_min)) gs_im = (gs_im - gs_min)/gs_max hsv_im = torch.stack((hue.float(), attn.squeeze().float(), gs_im.float()), -1) im = hsv_to_rgb(hsv_im.cpu().detach().numpy()) ex = make_grid(torch.tensor(im).permute(0,3,1,2), normalize=True, nrow=25) attns = make_grid(attn, normalize=False, nrow=25) attns = attns.cpu().detach() inputs = make_grid(x, normalize=True, nrow=25).cpu().detach() display.clear_output(wait=True) plt.figure(figsize=(20,8)) plt.imshow(np.concatenate((inputs.numpy().transpose(1,2,0),ex.numpy().transpose(1,2,0), attns.numpy().transpose(1,2,0)), axis=0)) #plt.xticks(np.linspace(18,324,10), classes) #plt.xticks(fontsize=20) plt.yticks([]) plt.title('CIFAR10 Epoch:{:02d}, Train:{:.3f}, Test:{:.3f}'.format(epoch, avg_acc, avg_acc_val), fontsize=20) display.display(plt.gcf()) fig, ax = plt.subplots(1,2, figsize=(20,8)) ax[0].set_title('Crossentropy') ax[0].plot(losses, label='Train') ax[0].plot(v_losses, label='CIFAR10 Test') ax[0].legend() ax[1].set_title('Accuracy') ax[1].plot(acces, label='Train') ax[1].plot(v_acces, label='CIFAR10 Test') ax[1].legend() display.display(plt.gcf())  I only trained it for 10 epochs here and get a passable performance, which does improve if it goes further. I stopped it to leave some of the mixed decisions the model is making. Declaring success here is premature for calling this as a great method for weak segmentation, but it does show exactly what the model considers spatially for every decision it makes. Once more, the model only uses values that are very positive thus saturating sigmoid to make a decision, by combining the argmax with the attention operation defined we can get the below visualization. The examples that show multiple colors are examples that share features with other classes, i.e. the birds and airplanes, deer and horses, cars and trucks. Now if there were only a method for looking at where all the pixels project into the learned space all at once!\n\nThough not absolutely necessary here's what the numbers look like during training. \n","date":1571270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571270400,"objectID":"e1cbf4aea1660dc89f0259975f1b8877","permalink":"https://charlielehman.github.io/post/weak-segmentation-cifar10/","publishdate":"2019-10-17T00:00:00Z","relpermalink":"/post/weak-segmentation-cifar10/","section":"post","summary":"Import Important Things import torch from torch import nn import torch.nn.functional as F from torch.utils.data import DataLoader from torchvision import transforms from torchvision.models import resnet18 from torchvision.datasets import CIFAR10 from tqdm import tqdm_notebook as tqdm from torchvision.utils import save_image, make_grid from matplotlib import pyplot as plt from matplotlib.colors import hsv_to_rgb from matplotlib.image import BboxImage from matplotlib.transforms import Bbox, TransformedBbox import numpy as np from IPython import display import requests from io import BytesIO from PIL import Image from PIL import Image, ImageSequence from IPython.","tags":null,"title":"Learning to segment CIFAR10","type":"post"},{"authors":["Charlie Lehman","Dogancan Temel","Ghassan AlRegib"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"b8401e4cb6180ab963cde58ab9c1edb0","permalink":"https://charlielehman.github.io/publication/implicit-background-estimation-for-semantic-segmentation/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/implicit-background-estimation-for-semantic-segmentation/","section":"publication","summary":"Scene understanding and semantic segmentation are at the core of many computer vision tasks, many of which, involve interacting with humans in potentially dangerous ways. It is therefore paramount that techniques for principled design of robust models be developed. In this paper, we provide analytic and empirical evidence that correcting potentially errant non-distinct mappings that result from the softmax function can result in improving robustness characteristics on a state-of-the-art semantic segmentation model with minimal impact to performance and minimal changes to the code base.","tags":["Source Themes"],"title":"Implicit Background Estimation for Semantic Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"I was interviewed by Julie Lindsay from Speechworks for a podcast regarding my experiences starting ConvexMind during the CreateX Startup Launch last Summer. There is also an article that summarizes the main points we discussed.\n","date":1565654400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565654400,"objectID":"ba87cce6b9ea62cd34c218cc45a13a8b","permalink":"https://charlielehman.github.io/post/speechworks-interview/","publishdate":"2019-08-13T00:00:00Z","relpermalink":"/post/speechworks-interview/","section":"post","summary":"I was interviewed by Julie Lindsay from Speechworks for a podcast regarding my experiences starting ConvexMind during the CreateX Startup Launch last Summer. There is also an article that summarizes the main points we discussed.","tags":null,"title":"Speechworks Interview","type":"post"},{"authors":null,"categories":null,"content":" I worked with Gukyeong Kwon and Jinsol Lee on this project for our Convex Optimization course. It was a neat project that really hits home that even if you can count cards perfectly\u0026hellip;the deck isn\u0026rsquo;t stacked in your favor. If you want to try it the code is linked above or if you want to run blackjacksim directly install it with:\npip3 install git+https://github.com/charlieLehman/blackjacksim  Tools from sklearn.metrics import confusion_matrix from sklearn.utils.multiclass import unique_labels import matplotlib from mpl_toolkits.axes_grid1 import make_axes_locatable %matplotlib inline from matplotlib import pyplot as plt def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues, **kwargs): \u0026quot;\u0026quot;\u0026quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \u0026quot;\u0026quot;\u0026quot; if not title: if normalize: title = 'Normalized confusion matrix' else: title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix(y_true, y_pred) # Only use the labels that appear in the data #classes = classes[unique_labels(y_true, y_pred)] if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(\u0026quot;Normalized confusion matrix\u0026quot;) else: print('Confusion matrix, without normalization') print(cm) fig, ax = plt.subplots(**kwargs) im = ax.imshow(cm, interpolation='nearest', cmap=cmap) divider = make_axes_locatable(ax) cax = divider.append_axes(\u0026quot;right\u0026quot;, size=\u0026quot;5%\u0026quot;, pad=0.05) ax.figure.colorbar(im, cax=cax) # We want to show all ticks... ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), # ... and label them with the respective list entries xticklabels=classes, yticklabels=classes, ylabel='True label', xlabel='Predicted label') # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=45, ha=\u0026quot;right\u0026quot;, rotation_mode=\u0026quot;anchor\u0026quot;) # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(j, i, format(cm[i, j], fmt), ha=\u0026quot;center\u0026quot;, va=\u0026quot;center\u0026quot;, color=\u0026quot;white\u0026quot; if cm[i, j] \u0026gt; thresh else \u0026quot;black\u0026quot;) fig.tight_layout() return ax  Comparison of House Rules from blackjacksim.simulations import Game from blackjacksim.entities import Shoe import matplotlib %matplotlib inline from tqdm import tnrange from tqdm import tqdm_notebook as tqdm from matplotlib import pyplot as plt import seaborn as sns import pandas as pd from jupyterthemes import jtplot jtplot.style(context='poster', fscale=1.4, spines=False, gridlines='--') from blackjacksim.data import DefaultGameConfig _def_conf = DefaultGameConfig() def config(house_rules): _def_conf['house']['class'] = house_rules return _def_conf try: df = df except: df = None pbar = tqdm(['Blackjack32', 'Blackjack65', 'Blackjack32NoSplit', 'Blackjack65NoSplit']) trials = 100 rounds = 100 for house in pbar: for i in range(trials): pbar.set_description(\u0026quot;{} {:04d}/{:04d}: \u0026quot;.format(house,i,trials-1)) g = Game(config(house)) for _ in range(rounds): g.round() if df is None: df = g.data else: df = pd.concat([df,g.data]) sns.lineplot(x='Round', y='Pool', hue='House', data=df) plt.show()  HBox(children=(IntProgress(value=0, max=4), HTML(value='')))  Modeling Action Strategy Build board state matrix A and action vector b from blackjacksim.entities import Deck, Hand from blackjacksim.strategies import basic import itertools import numpy as np import pandas as pd action_to_class = {'Hit':[1,0,0,0],'Stand':[0,1,0,0],'Split':[0,0,1,0],'Double':[0.0,0,0,1]} hands = [Hand(h) for h in itertools.product(Deck(),Deck())] dups = Deck() t = [] for hand, dup in itertools.product(hands, dups): tup = tuple(c.value for c in (*hand,dup)) c = (tup, basic(hand,dup)) if c not in t: t.append(c) print(len(t)) A = [] b = [] for a, _b in t: A.append(a) b.append(action_to_class[_b]) A = np.stack(A) b = np.array(b)  1001  Solve Least Squares import seaborn as sns from jupyterthemes import jtplot jtplot.style(context='paper', fscale=1.4, spines=False, gridlines='') A_ = np.concatenate([A, np.ones((A.shape[0],1))],1) Ai = np.linalg.pinv(A_) x = Ai@b out = A_@x pred = np.argmax(out,1) lab = np.argmax(b,1) lab_to_class = list(action_to_class.keys()) l2c = lambda x: lab_to_class[x] df = pd.DataFrame({'Prediction':pred, 'Label':lab,'HandSum':A[:,0:-1].sum(1), 'Hand':[a[0:-1] for a in A], 'Up Card':[a[-1] for a in A]}) df['Label Name'] = df.Label.apply(l2c) df['Prediction Name'] = df.Prediction.apply(l2c) df['Correct'] = df.Prediction == df.Label # Plot normalized confusion matrix classes = list(action_to_class.keys()) print('Accuracy: {:.2f}%\\n'.format(df.Correct.mean()*100)) plot_confusion_matrix(lab, pred, classes=classes, normalize=True, title=' ', figsize=(6,6)) plt.show()  Accuracy: 65.53% Normalized confusion matrix [[0.82826087 0.14782609 0. 0.02391304] [0.11551155 0.88448845 0. 0. ] [0.45833333 0.54166667 0. 0. ] [0.7 0.26315789 0. 0.03684211]]  Solve SVM with RBF kernel from sklearn.svm import SVC clf = SVC(gamma='auto', probability=True) label = b.argmax(1) clf.fit(A,label) print(clf.score(A,label)) pred = clf.predict(A) vals = clf.decision_function(A) probs = clf.predict_proba(A) classes = list(action_to_class.keys()) plot_confusion_matrix(label, pred, classes=classes, normalize=True, title=' ', figsize=(6,6)) plt.show()  0.932067932067932 Normalized confusion matrix [[0.9826087 0.00869565 0. 0.00869565] [0.00660066 0.98679868 0. 0.00660066] [0.41666667 0.45833333 0.10416667 0.02083333] [0.04736842 0.02105263 0. 0.93157895]]  Comparison of Optimizers for a Deep Model Train from blackjacksim.entities import Deck, Hand from blackjacksim.strategies import basic import itertools import numpy as np import pandas as pd import torch from torch import nn from tqdm import tnrange # Build A (Hand and Dealer's Up Card) and b (basic strategy Action) action_to_class = {'Hit':[1,0,0,0],'Stand':[0,1,0,0],'Split':[0,0,1,0],'Double':[0.0,0,0,1]} hands = [Hand(h) for h in itertools.product(Deck(),Deck())] dups = Deck() t = [] for hand, dup in itertools.product(hands, dups): tup = tuple(c.value for c in (*hand,dup)) c = (tup, basic(hand,dup)) if c not in t: t.append(c) print(len(t)) A = [] b = [] for a, _b in t: A.append(a) b.append(action_to_class[_b]) A = np.stack(A) b = np.array(b) A = torch.from_numpy(A).float() b = torch.from_numpy(b).float() # Build Deep Model class DeepBasicStrategy(nn.Module): def __init__(self): super(DeepBasicStrategy, self).__init__() block = lambda i, o: nn.Sequential( nn.Linear(i,o), nn.BatchNorm1d(o), nn.ReLU(), nn.Dropout(), ) _model = [] for i,o in [(3,2000), (2000,2000), (2000,1000), (1000,500), (500,250)]: _model.append(block(i,o)) _model.append(nn.Linear(250,4)) self.neural_net = nn.Sequential(*_model) def forward(self, x): return self.neural_net(x) A = A.cuda() b = b.cuda() # Train Deep Model criterion = nn.BCEWithLogitsLoss() train_log = [] for _ in tnrange(1, position=0): for opt_name in ['SGD', 'SGD w/ momentum', 'SGD w/ Nesterov momentum', 'Adam']: model = DeepBasicStrategy() model = model.cuda() closure = None if opt_name == 'SGD': optimizer = torch.optim.SGD(model.parameters(), lr=1.0) elif opt_name == 'SGD w/ momentum': optimizer = torch.optim.SGD(model.parameters(), lr=1.0, momentum=0.9) elif opt_name == 'SGD w/ Nesterov momentum': optimizer = torch.optim.SGD(model.parameters(), lr=1.0, momentum=0.9, nesterov=True) elif opt_name == 'Adam': optimizer = torch.optim.Adam(model.parameters()) elif opt_name == 'LBFGS': optimizer = torch.optim.LBFGS(model.parameters(), lr=1.0)#, history_size=100, max_iter=3, max_eval=4) closure = lambda: criterion(model(A),b) tbar = tnrange(1000, position=1) for step in tbar: optimizer.zero_grad() model.train() out = model(A) loss = criterion(out, b) model.eval() out = model(A) pred = out.argmax(1) label = b.argmax(1) acc = (pred==label).float().mean().item() tbar.set_description(\u0026quot;BCE Loss: {:.3f} Acc: {:.3f}\u0026quot;.format(loss.item(), acc)) loss.backward() optimizer.step(closure) train_log.append( { 'Optimizer':opt_name, 'Step':step, 'Accuracy':acc, 'Loss':loss.item(), } )  1001 HBox(children=(IntProgress(value=0, max=1), HTML(value=''))) HBox(children=(IntProgress(value=0, max=1000), HTML(value=''))) HBox(children=(IntProgress(value=0, max=1000), HTML(value=''))) HBox(children=(IntProgress(value=0, max=1000), HTML(value=''))) HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))  Plot loss and accuracy import matplotlib %matplotlib inline from matplotlib import pyplot as plt import seaborn as sns from jupyterthemes import jtplot train_df = pd.DataFrame(train_log) train_df['Error'] = 1-train_df.Accuracy # set \u0026quot;context\u0026quot; (paper, notebook, talk, poster) # scale font-size of ticklabels, legend, etc. # remove spines from x and y axes and make grid dashed jtplot.style(context='paper', fscale=1.4, spines=False, gridlines='--') fig,ax = plt.subplots(1, figsize=(7, 5)) ax.set(yscale='log') sns.lineplot(x='Step', y='Loss', hue='Optimizer', data=train_df, ax=ax) plt.show() fig,ax = plt.subplots(1, figsize=(7, 5)) ax.set(ylim=[.6,1.05]) sns.lineplot(x='Step', y='Accuracy', hue='Optimizer', data=train_df, ax=ax) plt.show() jtplot.style(context='paper', fscale=1.4, spines=False, gridlines='') classes = list(action_to_class.keys()) plot_confusion_matrix(label.cpu(), pred.cpu(), classes=classes, normalize=True, title=' ', figsize=(6,6)) plt.show()  Normalized confusion matrix [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]]  Visualization of A, RBF, and Deep Model representations from sklearn.decomposition import PCA a = A.cpu().detach().numpy() pca = PCA(2) y = pca.fit_transform(a) l = label.cpu().detach().numpy() p = pred.cpu().detach().numpy() X = out.cpu().detach().numpy() fig, ax = plt.subplots(1,3, figsize=(15,5)) ax[0].scatter(y[l==0,0], y[l==0,1], label='Hit') ax[0].scatter(y[l==1,0], y[l==1,1], label='Stand') ax[0].scatter(y[l==2,0], y[l==2,1], label='Split') ax[0].scatter(y[l==3,0], y[l==3,1], label='Double') ax[0].set_xticks([]) ax[0].set_yticks([]) ax[0].set_title('Raw') pca = PCA(2) y = pca.fit_transform(X) ax[2].scatter(y[l==0,0], y[l==0,1], label='Hit') ax[2].scatter(y[l==1,0], y[l==1,1], label='Stand') ax[2].scatter(y[l==2,0], y[l==2,1], label='Split') ax[2].scatter(y[l==3,0], y[l==3,1], label='Double') ax[2].set_xticks([]) ax[2].set_yticks([]) ax[2].set_title('Deep Model') pca = PCA(2) y = pca.fit_transform(vals) hit = ax[1].scatter(y[l==0,0], y[l==0,1], label='Hit') stand = ax[1].scatter(y[l==1,0], y[l==1,1], label='Stand') split = ax[1].scatter(y[l==2,0], y[l==2,1], label='Split') double = ax[1].scatter(y[l==3,0], y[l==3,1], label='Double') ax[1].set_xticks([]) ax[1].set_yticks([]) ax[1].set_title('RBF Kernel') fig.legend(['Hit','Stand','Split','Double'], bbox_to_anchor=[0.39, 0.05], loc='center', ncol=4) plt.show()  ","date":1551225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551225600,"objectID":"f57e1a9362c22a7ebc3730f8beef5842","permalink":"https://charlielehman.github.io/project/card-counting/","publishdate":"2019-02-27T00:00:00Z","relpermalink":"/project/card-counting/","section":"project","summary":"Project for ECE8823 Convex Optimization course at Georgia Tech","tags":["Machine Learning","Class Project"],"title":"Blackjack Simulator","type":"project"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://charlielehman.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Pixelwise classifiers with Large-margin Gaussian Mixture representation These are several visualizations I dug up from really really messy code, which I won't share just yet. This was some quick and dirty work to test out the feasibility of an idea: I wanted to visualize the effects on the representation space at each pixel during an FGSM attack. I trained a weak segmentor model with the representation space modified for L-GM. I found L-GM useful because it provides a sensible way of directly learning a representation that can be visualized without further solving an additional optimization problem (looking at you T-SNE!). Not to mention you can project new points into it, which was really central to my purpose of observing the effects of an adversarial attack. Original Truck Image This is what a normal input from the CIFAR10 test set looks like.\nThe max-softmax, which is a good baseline for confidence in detection tasks drops from 1.00 to 0.72 after adversarial modification. I also computed a confidence term that looked for regions of \u0026quot;uncertainty\u0026quot;, which I am using loosely. Specifically I was interested in regions close to 0.5 when using sigmoid on the raw logits and the logits post temperature scaling. I saw that when the model is behaving well there is a uniform expansion of the uncertain regions spatially resulting in little overlap between the raw and temperature scaled regions. So by multiplying both taking the spatial average I can determine the overall uncertainy and by additive inverse get a confidence measure for observing adversarial images spatially. Original Truck Image This is what an untargeted FGSM attack looks like.\nIn this case it appears that FGSM just messed with the image's high contrast regions on the cab of the truck. My confidence measure drops to 0.64 from 0.92, but the detection still clears the 0 threshold sigmoid learned.\nWhen CLAHE is applied to both images we can see some interesting effects.\nOriginal Truck Image\nOn the original image, my confidence measure drops from 0.92 to 0.84 and the pixel embedding spreads out far more.\nOriginal Truck Image\nOn the FGSM image, my confidence measure plummets from 0.64 to 0.21. Interestingly, the pixel embedding stretches back toward truck.\n","date":1544659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544659200,"objectID":"77db529b52675b73adf672146bfc83a7","permalink":"https://charlielehman.github.io/post/visualizing-fgsm/","publishdate":"2018-12-13T00:00:00Z","relpermalink":"/post/visualizing-fgsm/","section":"post","summary":"Pixelwise classifiers with Large-margin Gaussian Mixture representation These are several visualizations I dug up from really really messy code, which I won't share just yet. This was some quick and dirty work to test out the feasibility of an idea: I wanted to visualize the effects on the representation space at each pixel during an FGSM attack. I trained a weak segmentor model with the representation space modified for L-GM. I found L-GM useful because it provides a sensible way of directly learning a representation that can be visualized without further solving an additional optimization problem (looking at you T-SNE!","tags":null,"title":"Visualizing Adversarial Attacks","type":"post"},{"authors":null,"categories":null,"content":"I spent some time going through Tensorflow tutorials and was curious what the learned kernels in the convolution layers looked like. After getting a visulization to work for regular RGB input images from CIFAR10, I transformed and created copies of the dataset using other representations: FFT, DCT, and HSV. The same model used to train the RGB inputs was used and similar performance was achieved.\nIf you would like to learn more you can implement it yourself here\n","date":1485820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485820800,"objectID":"2e69ff0d5ec35715a1a4034f3e6b8cf8","permalink":"https://charlielehman.github.io/post/multi-basis-input/","publishdate":"2017-01-31T00:00:00Z","relpermalink":"/post/multi-basis-input/","section":"post","summary":"I spent some time going through Tensorflow tutorials and was curious what the learned kernels in the convolution layers looked like. After getting a visulization to work for regular RGB input images from CIFAR10, I transformed and created copies of the dataset using other representations: FFT, DCT, and HSV. The same model used to train the RGB inputs was used and similar performance was achieved.\nIf you would like to learn more you can implement it yourself here","tags":null,"title":"Multi-Basis Input Convolutional Neural Network","type":"post"},{"authors":["Robert J. Barsanti","Charlie Lehman"],"categories":null,"content":"","date":1204329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1204329600,"objectID":"4bf32232fcb6d91745426bad822c1fc9","permalink":"https://charlielehman.github.io/publication/application-of-a-wavelet-based-receiver-for-the-coherent-detection-of-fsk-signals/","publishdate":"2008-03-01T00:00:00Z","relpermalink":"/publication/application-of-a-wavelet-based-receiver-for-the-coherent-detection-of-fsk-signals/","section":"publication","summary":"This paper investigates the application of wavelet analysis to the problem of coherent detection of digital binary frequency shift keying communication signals in additive white Gaussian noise channels. The proposed wavelet-based receiver computes the normalized cross correlation between the filtered wavelet coefficients of the received signal and wavelet coefficients that correspond to the known transmitted FSK signals. Simulations are conducted comparing the wavelet receiver to the classical coherent FSK receiver.","tags":["Source Themes"],"title":"Application of a Wavelet-Based Receiver for the Coherent Detection of FSK Signals","type":"publication"}]