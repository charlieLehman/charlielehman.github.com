<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Charlie Lehman</title>
    <link>https://charlielehman.github.io/</link>
      <atom:link href="https://charlielehman.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Charlie Lehman</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://charlielehman.github.io/img/icon-192.png</url>
      <title>Charlie Lehman</title>
      <link>https://charlielehman.github.io/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>https://charlielehman.github.io/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://charlielehman.github.io/courses/example/example1/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>https://charlielehman.github.io/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://charlielehman.github.io/courses/example/example2/</guid>
      <description>

&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;

&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://charlielehman.github.io/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to segment CIFAR10</title>
      <link>https://charlielehman.github.io/post/weak-segmentation-cifar10/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/weak-segmentation-cifar10/</guid>
      <description>&lt;h2 id=&#34;import-important-things&#34;&gt;Import Important Things&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.models import resnet18
from torchvision.datasets import CIFAR10
from tqdm import tqdm_notebook as tqdm
from torchvision.utils import save_image, make_grid
from matplotlib import pyplot as plt
from matplotlib.colors import hsv_to_rgb
from matplotlib.image import BboxImage
from matplotlib.transforms import Bbox, TransformedBbox
import numpy as np
from IPython import display
import requests
from io import BytesIO
from PIL import Image
from PIL import Image, ImageSequence
from IPython.display import HTML
import warnings
from matplotlib import rc
import gc
import matplotlib
matplotlib.rcParams[&#39;pdf.fonttype&#39;] = 42
matplotlib.rcParams[&#39;ps.fonttype&#39;] = 42
gc.enable()
plt.ioff()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;initialize-the-tiny-model-from-resnet18&#34;&gt;Initialize the tiny model from ResNet18&lt;/h2&gt;

&lt;p&gt;I am replacing the first 7x7 conv stride of 4 with a 3x3 convolution kernel with stride of 1 and replacing maxpool with upsample.  This keeps the spatial features from being downsampled too quickly as the forward pass propagates.  The linear layer is replaced with a &amp;quot;pixelwise linear layer&amp;quot;, or a 1x1 convolution with stride of 1.  This can be simply thought of as projecting a 1x512 vector (pixel) with a 512x10 matrix (1x1 conv). Notice that there is no operation that performs a spatial aggregation so what we have left is a 10x32x32 tensor after the final upsample.  This can be used the same way as a semantic segmentation output, which we can also aggregate spatially and optimize using image level labels.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;num_classes = 10
resnet = resnet18(pretrained=True)
resnet.conv1 = nn.Conv2d(3,64,3,stride=1,padding=1)
resnet_ = list(resnet.children())[:-2]
resnet_[3] = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=False)
classifier = nn.Conv2d(512,num_classes,1)
torch.nn.init.kaiming_normal_(classifier.weight)
resnet_.append(classifier)
resnet_.append(nn.Upsample(size=32, mode=&#39;bilinear&#39;, align_corners=False))
tiny_resnet = nn.Sequential(*resnet_)

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;define-attention&#34;&gt;Define Attention&lt;/h2&gt;

&lt;p&gt;In short, I&#39;m going to just define 0 as the threshold in the logit (pre-softmax space).  By selecting the largest component of the logit vector and then running it through sigmoid we can get a value with a support from 0 to 1, which is useful for inspecting the &amp;quot;attention&amp;quot; of the model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def attention(x):
    return torch.sigmoid(torch.logsumexp(x,1, keepdim=True))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cifar10-dataset&#34;&gt;CIFAR10 dataset&lt;/h2&gt;

&lt;p&gt;This dataset is so convenient for demonstrating so many things.  There are much more impressive demonstrations of weak segmentation, but all of this can be accomplished in a jupyter notebook so here we go!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=8),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

trainset = CIFAR10(root=&#39;.&#39;, train=True, download=True, transform=transform_train)
train_iter = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)

testset = CIFAR10(root=&#39;.&#39;, train=False, download=True, transform=transform_test)
test_iter = DataLoader(testset, batch_size=100, shuffle=False, num_workers=16, pin_memory=True)

classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Files already downloaded and verified
Files already downloaded and verified
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;train-and-visualize&#34;&gt;Train and Visualize&lt;/h2&gt;

&lt;p&gt;The key take aways from the below code is that the objective for optimization is Binary Cross Entropy and the model&#39;s spatial aggregation is accomplished with a smooth-max operation.  This means after aggregation the vector is optimized to be a set of 10 binary detectors, which is in contrast to the most popular method of characterization: softmax cross entropy, which encourages each pixel to select only one.  When combined with the aforementioned attention operation we can forego aggregation and directly inspect exactly what the model uses to make a decision!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = nn.DataParallel(tiny_resnet).cuda()
num_epochs = 10
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9, weight_decay=1e-4)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,78,eta_min=0.001)

losses = []
acces = []
v_losses = []
v_acces = []
for epoch in tqdm(range(num_epochs)):
    epoch_loss = 0.0
    acc = 0.0
    var = 0.0
    model.train()
    train_pbar = train_iter
    for i, (x, _label) in enumerate(train_pbar):
        x = x.cuda()
        _label = _label.cuda()
        label = F.one_hot(_label).float()
        seg_out = model(x)
        
        attn = attention(seg_out)
        # Smooth Max Aggregation
        logit = torch.log(torch.exp(seg_out*0.5).mean((-2,-1)))*2
        loss = criterion(logit, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        epoch_loss += loss.item()
        acc += (logit.argmax(-1)==_label).sum()
        #train_pbar.set_description(&#39;Accuracy: {:.3f}%&#39;.format(100*(logit.argmax(-1)==_label).float().mean()))
        
    avg_loss = epoch_loss / (i + 1)
    losses.append(avg_loss)
    avg_acc = acc.cpu().detach().numpy() / (len(trainset))
    acces.append(avg_acc)
    model.eval()
    epoch_loss = 0.0
    acc = 0.0
    num_seen = 0
    
    test_pbar = tqdm(test_iter)
    for i, (x, _label) in enumerate(test_pbar):
        x = x.cuda()
        _label = _label.cuda()
        label = F.one_hot(_label).float()
        seg_out = model(x)
        attn = attention(seg_out)
        logit = torch.log(torch.exp(seg_out*0.5).mean((-2,-1)))*2
        loss = criterion(logit, label)
        epoch_loss += loss.item()
        acc += (logit.argmax(-1)==_label).sum()
        num_seen += label.size(0)
        test_pbar.set_description(&#39;Accuracy: {:.3f}%&#39;.format(100*(acc.float()/num_seen)))
    
    avg_loss_val = epoch_loss / (i + 1)
    v_losses.append(avg_loss_val)
    avg_acc_val = acc.cpu().detach().numpy() / (len(testset))
    v_acces.append(avg_acc_val)
    plt.close(&#39;all&#39;)

    conf = torch.max(nn.functional.softmax(seg_out, dim=1), dim=1)[0]
    hue = (torch.argmax(seg_out, dim=1).float() + 0.5)/10
    x -= x.min()
    x /= x.max()
    gs_im = x.mean(1)
    gs_mean = gs_im.mean()
    gs_min = gs_im.min()
    gs_max = torch.max((gs_im-gs_min))
    gs_im = (gs_im - gs_min)/gs_max
    hsv_im = torch.stack((hue.float(), attn.squeeze().float(), gs_im.float()), -1)
    im = hsv_to_rgb(hsv_im.cpu().detach().numpy())
    ex = make_grid(torch.tensor(im).permute(0,3,1,2), normalize=True, nrow=25)
    attns = make_grid(attn, normalize=False, nrow=25)
    attns = attns.cpu().detach()
    inputs = make_grid(x, normalize=True, nrow=25).cpu().detach()
    display.clear_output(wait=True)
    plt.figure(figsize=(20,8))
    plt.imshow(np.concatenate((inputs.numpy().transpose(1,2,0),ex.numpy().transpose(1,2,0), attns.numpy().transpose(1,2,0)), axis=0))
    #plt.xticks(np.linspace(18,324,10), classes)
    #plt.xticks(fontsize=20) 
    plt.yticks([])
    plt.title(&#39;CIFAR10 Epoch:{:02d}, Train:{:.3f}, Test:{:.3f}&#39;.format(epoch, avg_acc, avg_acc_val), fontsize=20)
    display.display(plt.gcf())
    fig, ax = plt.subplots(1,2, figsize=(20,8))
    ax[0].set_title(&#39;Crossentropy&#39;)
    ax[0].plot(losses, label=&#39;Train&#39;)
    ax[0].plot(v_losses, label=&#39;CIFAR10 Test&#39;)
    ax[0].legend()
    ax[1].set_title(&#39;Accuracy&#39;)
    ax[1].plot(acces, label=&#39;Train&#39;)
    ax[1].plot(v_acces, label=&#39;CIFAR10 Test&#39;)
    ax[1].legend()
    display.display(plt.gcf())
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I only trained it for 10 epochs here and get a passable performance, which does improve if it goes further.  I stopped it to leave some of the mixed decisions the model is making.  Declaring success here is premature for calling this as a great method for weak segmentation, but it does show exactly what the model considers spatially for every decision it makes.  Once more, the model only uses values that are very positive thus saturating sigmoid to make a decision, by combining the argmax with the attention operation defined we can get the below visualization.  The examples that show multiple colors are examples that share features with other classes, i.e. the birds and airplanes, deer and horses, cars and trucks.  Now if there were only a method for looking at where all the pixels project into the learned space all at once!&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./Weak_Segmentation_of_CIFAR10_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Though not absolutely necessary here&#39;s what the numbers look like during training.
&lt;figure&gt;&lt;img src=&#34;./Weak_Segmentation_of_CIFAR10_7_1.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implicit Background Estimation for Semantic Segmentation</title>
      <link>https://charlielehman.github.io/publication/implicit-background-estimation-for-semantic-segmentation/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/publication/implicit-background-estimation-for-semantic-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Speechworks Interview</title>
      <link>https://charlielehman.github.io/post/speechworks-interview/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/speechworks-interview/</guid>
      <description>&lt;p&gt;I was interviewed by Julie Lindsay from Speechworks for a &lt;a href=&#34;https://www.speechworks.net/podcast/podcast-the-entrepreneurs-secret-why-a-perfect-pitch-isnt-just-about-ideas/&#34;&gt;podcast&lt;/a&gt; regarding my experiences starting ConvexMind during the CreateX Startup Launch last Summer. There is also an &lt;a href=&#34;https://www.speechworks.net/the-entrepreneurs-secret-why-a-perfect-pitch-isnt-just-about-ideas/&#34;&gt;article&lt;/a&gt; that summarizes the main points we discussed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blackjack Simulator</title>
      <link>https://charlielehman.github.io/project/card-counting/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/project/card-counting/</guid>
      <description>

&lt;p&gt;I worked with Gukyeong Kwon and Jinsol Lee on this project for our Convex Optimization course.  It was a neat project that really hits home that even if you can count cards perfectly&amp;hellip;the deck isn&amp;rsquo;t stacked in your favor.  If you want to try it the code is linked above or if you want to run blackjacksim directly install it with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install git+https://github.com/charlieLehman/blackjacksim
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;tools&#34;&gt;Tools&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels
import matplotlib
from mpl_toolkits.axes_grid1 import make_axes_locatable
%matplotlib inline
from matplotlib import pyplot as plt

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues,
                         **kwargs):
    &amp;quot;&amp;quot;&amp;quot;
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    &amp;quot;&amp;quot;&amp;quot;
    if not title:
        if normalize:
            title = &#39;Normalized confusion matrix&#39;
        else:
            title = &#39;Confusion matrix, without normalization&#39;

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    #classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis]
        print(&amp;quot;Normalized confusion matrix&amp;quot;)
    else:
        print(&#39;Confusion matrix, without normalization&#39;)

    print(cm)

    fig, ax = plt.subplots(**kwargs)
    im = ax.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap)
    divider = make_axes_locatable(ax)
    cax = divider.append_axes(&amp;quot;right&amp;quot;, size=&amp;quot;5%&amp;quot;, pad=0.05)
    ax.figure.colorbar(im, cax=cax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           ylabel=&#39;True label&#39;,
           xlabel=&#39;Predicted label&#39;)

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha=&amp;quot;right&amp;quot;,
             rotation_mode=&amp;quot;anchor&amp;quot;)

    # Loop over data dimensions and create text annotations.
    fmt = &#39;.2f&#39; if normalize else &#39;d&#39;
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha=&amp;quot;center&amp;quot;, va=&amp;quot;center&amp;quot;,
                    color=&amp;quot;white&amp;quot; if cm[i, j] &amp;gt; thresh else &amp;quot;black&amp;quot;)
            

    
    fig.tight_layout()
    return ax
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;comparison-of-house-rules&#34;&gt;Comparison of House Rules&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from blackjacksim.simulations import Game
from blackjacksim.entities import Shoe
import matplotlib
%matplotlib inline
from tqdm import tnrange
from tqdm import tqdm_notebook as tqdm
from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
from jupyterthemes import jtplot
jtplot.style(context=&#39;poster&#39;, fscale=1.4, spines=False, gridlines=&#39;--&#39;)

from blackjacksim.data import DefaultGameConfig
_def_conf = DefaultGameConfig()
def config(house_rules):
    _def_conf[&#39;house&#39;][&#39;class&#39;] = house_rules
    return _def_conf

try:
    df = df
except:
    df = None
    
pbar = tqdm([&#39;Blackjack32&#39;, &#39;Blackjack65&#39;, &#39;Blackjack32NoSplit&#39;, &#39;Blackjack65NoSplit&#39;])
trials = 100
rounds = 100
for house in pbar:
    for i in range(trials):
        pbar.set_description(&amp;quot;{} {:04d}/{:04d}: &amp;quot;.format(house,i,trials-1))
        g = Game(config(house))
        for _ in range(rounds):
            g.round()
        if df is None:
            df = g.data 
        else:
            df = pd.concat([df,g.data]) 
        
sns.lineplot(x=&#39;Round&#39;, y=&#39;Pool&#39;, hue=&#39;House&#39;, data=df)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;HBox(children=(IntProgress(value=0, max=4), HTML(value=&#39;&#39;)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./blackjacksim_project_5_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;modeling-action-strategy&#34;&gt;Modeling Action Strategy&lt;/h1&gt;

&lt;h2 id=&#34;build-board-state-matrix-a-and-action-vector-b&#34;&gt;Build board state matrix A and action vector b&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from blackjacksim.entities import Deck, Hand
from blackjacksim.strategies import basic
import itertools
import numpy as np
import pandas as pd

action_to_class = {&#39;Hit&#39;:[1,0,0,0],&#39;Stand&#39;:[0,1,0,0],&#39;Split&#39;:[0,0,1,0],&#39;Double&#39;:[0.0,0,0,1]}

hands = [Hand(h) for h in itertools.product(Deck(),Deck())]
dups = Deck()

t = []
for hand, dup  in itertools.product(hands, dups):
    tup = tuple(c.value for c in (*hand,dup))
    c = (tup, basic(hand,dup))
    if c not in t:
        t.append(c)
print(len(t))
A = []
b = []
for  a, _b in t:
    A.append(a)
    b.append(action_to_class[_b])
    
A = np.stack(A)
b = np.array(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1001
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;solve-least-squares&#34;&gt;Solve Least Squares&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import seaborn as sns
from jupyterthemes import jtplot
jtplot.style(context=&#39;paper&#39;, fscale=1.4, spines=False, gridlines=&#39;&#39;)

A_ = np.concatenate([A, np.ones((A.shape[0],1))],1)
Ai = np.linalg.pinv(A_)
x = Ai@b
out = A_@x
pred = np.argmax(out,1)
lab = np.argmax(b,1)

lab_to_class = list(action_to_class.keys())
l2c = lambda x: lab_to_class[x]

df = pd.DataFrame({&#39;Prediction&#39;:pred, &#39;Label&#39;:lab,&#39;HandSum&#39;:A[:,0:-1].sum(1), &#39;Hand&#39;:[a[0:-1] for a in A], &#39;Up Card&#39;:[a[-1] for a in A]})
df[&#39;Label Name&#39;] = df.Label.apply(l2c)
df[&#39;Prediction Name&#39;] = df.Prediction.apply(l2c)
df[&#39;Correct&#39;] = df.Prediction == df.Label

# Plot normalized confusion matrix
classes = list(action_to_class.keys())
print(&#39;Accuracy: {:.2f}%\n&#39;.format(df.Correct.mean()*100))
plot_confusion_matrix(lab, pred, classes=classes, normalize=True, title=&#39; &#39;, figsize=(6,6))
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Accuracy: 65.53%

Normalized confusion matrix
[[0.82826087 0.14782609 0.         0.02391304]
 [0.11551155 0.88448845 0.         0.        ]
 [0.45833333 0.54166667 0.         0.        ]
 [0.7        0.26315789 0.         0.03684211]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./blackjacksim_project_10_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;solve-svm-with-rbf-kernel&#34;&gt;Solve SVM with RBF kernel&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.svm import SVC
clf = SVC(gamma=&#39;auto&#39;, probability=True)
label = b.argmax(1)
clf.fit(A,label)
print(clf.score(A,label))
pred = clf.predict(A)

vals = clf.decision_function(A)
probs = clf.predict_proba(A)

classes = list(action_to_class.keys())
plot_confusion_matrix(label, pred, classes=classes, normalize=True, title=&#39; &#39;, figsize=(6,6))
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.932067932067932
Normalized confusion matrix
[[0.9826087  0.00869565 0.         0.00869565]
 [0.00660066 0.98679868 0.         0.00660066]
 [0.41666667 0.45833333 0.10416667 0.02083333]
 [0.04736842 0.02105263 0.         0.93157895]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./blackjacksim_project_12_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;comparison-of-optimizers-for-a-deep-model&#34;&gt;Comparison of Optimizers for a Deep Model&lt;/h2&gt;

&lt;h3 id=&#34;train&#34;&gt;Train&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from blackjacksim.entities import Deck, Hand
from blackjacksim.strategies import basic
import itertools
import numpy as np
import pandas as pd
import torch
from torch import nn
from tqdm import tnrange


# Build A (Hand and Dealer&#39;s Up Card) and b (basic strategy Action)
action_to_class = {&#39;Hit&#39;:[1,0,0,0],&#39;Stand&#39;:[0,1,0,0],&#39;Split&#39;:[0,0,1,0],&#39;Double&#39;:[0.0,0,0,1]}

hands = [Hand(h) for h in itertools.product(Deck(),Deck())]
dups = Deck()

t = []
for hand, dup  in itertools.product(hands, dups):
    tup = tuple(c.value for c in (*hand,dup))
    c = (tup, basic(hand,dup))
    if c not in t:
        t.append(c)
print(len(t))
A = []
b = []
for  a, _b in t:
    A.append(a)
    b.append(action_to_class[_b])
    
A = np.stack(A)
b = np.array(b)

A = torch.from_numpy(A).float()
b = torch.from_numpy(b).float()

# Build Deep Model
class DeepBasicStrategy(nn.Module):
    def __init__(self):
        super(DeepBasicStrategy, self).__init__()
        block = lambda i, o: nn.Sequential(
            nn.Linear(i,o),
            nn.BatchNorm1d(o),
            nn.ReLU(),
            nn.Dropout(),
        )

        _model = []
        for i,o in [(3,2000), (2000,2000), (2000,1000), (1000,500), (500,250)]:
            _model.append(block(i,o))
        _model.append(nn.Linear(250,4))
        self.neural_net = nn.Sequential(*_model)
    def forward(self, x):
        return self.neural_net(x)

A = A.cuda()
b = b.cuda()

# Train Deep Model
criterion = nn.BCEWithLogitsLoss()
train_log = []
for _ in tnrange(1, position=0):
    for opt_name in [&#39;SGD&#39;, &#39;SGD w/ momentum&#39;, &#39;SGD w/ Nesterov momentum&#39;, &#39;Adam&#39;]:
        model = DeepBasicStrategy()
        model = model.cuda()
        closure = None
        if opt_name == &#39;SGD&#39;:
            optimizer = torch.optim.SGD(model.parameters(), lr=1.0)
        elif opt_name == &#39;SGD w/ momentum&#39;:
            optimizer = torch.optim.SGD(model.parameters(), lr=1.0, momentum=0.9)
        elif opt_name == &#39;SGD w/ Nesterov momentum&#39;:
            optimizer = torch.optim.SGD(model.parameters(), lr=1.0, momentum=0.9, nesterov=True)
        elif opt_name == &#39;Adam&#39;:
            optimizer = torch.optim.Adam(model.parameters())
        elif opt_name == &#39;LBFGS&#39;:
            optimizer = torch.optim.LBFGS(model.parameters(), lr=1.0)#, history_size=100, max_iter=3, max_eval=4)
            closure = lambda: criterion(model(A),b)
        tbar = tnrange(1000, position=1)
        for step in tbar:
            optimizer.zero_grad()
            model.train()
            out = model(A)
            loss = criterion(out, b)
            model.eval()
            out = model(A)
            pred = out.argmax(1)
            label = b.argmax(1)
            acc = (pred==label).float().mean().item()
            tbar.set_description(&amp;quot;BCE Loss: {:.3f} Acc: {:.3f}&amp;quot;.format(loss.item(), acc))
            loss.backward()
            optimizer.step(closure)
            train_log.append(
                {
                    &#39;Optimizer&#39;:opt_name,
                    &#39;Step&#39;:step,
                    &#39;Accuracy&#39;:acc,
                    &#39;Loss&#39;:loss.item(),
                }
            )

        
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1001



HBox(children=(IntProgress(value=0, max=1), HTML(value=&#39;&#39;)))



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))



HBox(children=(IntProgress(value=0, max=1000), HTML(value=&#39;&#39;)))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;plot-loss-and-accuracy&#34;&gt;Plot loss and accuracy&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib
%matplotlib inline
from matplotlib import pyplot as plt
import seaborn as sns
from jupyterthemes import jtplot

train_df = pd.DataFrame(train_log)
train_df[&#39;Error&#39;] = 1-train_df.Accuracy
# set &amp;quot;context&amp;quot; (paper, notebook, talk, poster)
# scale font-size of ticklabels, legend, etc.
# remove spines from x and y axes and make grid dashed
jtplot.style(context=&#39;paper&#39;, fscale=1.4, spines=False, gridlines=&#39;--&#39;)
fig,ax = plt.subplots(1, figsize=(7, 5))
ax.set(yscale=&#39;log&#39;)
sns.lineplot(x=&#39;Step&#39;, y=&#39;Loss&#39;, hue=&#39;Optimizer&#39;, data=train_df, ax=ax)
plt.show()
fig,ax = plt.subplots(1, figsize=(7, 5))
ax.set(ylim=[.6,1.05])
sns.lineplot(x=&#39;Step&#39;, y=&#39;Accuracy&#39;, hue=&#39;Optimizer&#39;, data=train_df, ax=ax)
plt.show()

jtplot.style(context=&#39;paper&#39;, fscale=1.4, spines=False, gridlines=&#39;&#39;)
classes = list(action_to_class.keys())
plot_confusion_matrix(label.cpu(), pred.cpu(), classes=classes, normalize=True, title=&#39; &#39;, figsize=(6,6))
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./blackjacksim_project_17_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./blackjacksim_project_17_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Normalized confusion matrix
[[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./blackjacksim_project_17_3.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;visualization-of-a-rbf-and-deep-model-representations&#34;&gt;Visualization of A, RBF, and Deep Model representations&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.decomposition import PCA
a = A.cpu().detach().numpy()
pca = PCA(2)
y = pca.fit_transform(a)
l = label.cpu().detach().numpy()
p = pred.cpu().detach().numpy()
X = out.cpu().detach().numpy()
fig, ax = plt.subplots(1,3, figsize=(15,5))
ax[0].scatter(y[l==0,0], y[l==0,1], label=&#39;Hit&#39;)
ax[0].scatter(y[l==1,0], y[l==1,1], label=&#39;Stand&#39;)
ax[0].scatter(y[l==2,0], y[l==2,1], label=&#39;Split&#39;)
ax[0].scatter(y[l==3,0], y[l==3,1], label=&#39;Double&#39;)
ax[0].set_xticks([])
ax[0].set_yticks([])
ax[0].set_title(&#39;Raw&#39;)
pca = PCA(2)
y = pca.fit_transform(X)
ax[2].scatter(y[l==0,0], y[l==0,1], label=&#39;Hit&#39;)
ax[2].scatter(y[l==1,0], y[l==1,1], label=&#39;Stand&#39;)
ax[2].scatter(y[l==2,0], y[l==2,1], label=&#39;Split&#39;)
ax[2].scatter(y[l==3,0], y[l==3,1], label=&#39;Double&#39;)
ax[2].set_xticks([])
ax[2].set_yticks([])
ax[2].set_title(&#39;Deep Model&#39;)
pca = PCA(2)
y = pca.fit_transform(vals)
hit = ax[1].scatter(y[l==0,0], y[l==0,1], label=&#39;Hit&#39;)
stand = ax[1].scatter(y[l==1,0], y[l==1,1], label=&#39;Stand&#39;)
split = ax[1].scatter(y[l==2,0], y[l==2,1], label=&#39;Split&#39;)
double = ax[1].scatter(y[l==3,0], y[l==3,1], label=&#39;Double&#39;)
ax[1].set_xticks([])
ax[1].set_yticks([])
ax[1].set_title(&#39;RBF Kernel&#39;)
fig.legend([&#39;Hit&#39;,&#39;Stand&#39;,&#39;Split&#39;,&#39;Double&#39;], bbox_to_anchor=[0.39, 0.05],  loc=&#39;center&#39;, ncol=4)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./blackjacksim_project_19_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://charlielehman.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Adversarial Attacks</title>
      <link>https://charlielehman.github.io/post/visualizing-fgsm/</link>
      <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/visualizing-fgsm/</guid>
      <description>&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;orig.png&#34; alt=&#34;Original&#34; title=&#34;Original Truck Image&#34;&gt;&lt;figcaption&gt;Original Truck Image&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;img src=&#34;fgsm.png&#34; alt=&#34;FGSM&#34; title=&#34;Original Truck Image&#34;&gt;&lt;figcaption&gt;Original Truck Image&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;img src=&#34;orig-eqhist.png&#34; alt=&#34;Original with CLAHE&#34; title=&#34;Original Truck Image&#34;&gt;&lt;figcaption&gt;Original Truck Image&lt;/figcaption&gt;&lt;/figure&gt;
&lt;figure&gt;&lt;img src=&#34;fgsm-eqhist.png&#34; alt=&#34;FGSM with CLAHE&#34; title=&#34;Original Truck Image&#34;&gt;&lt;figcaption&gt;Original Truck Image&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Basis Input Convolutional Neural Network</title>
      <link>https://charlielehman.github.io/post/multi-basis-input/</link>
      <pubDate>Tue, 31 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/multi-basis-input/</guid>
      <description>&lt;p&gt;I spent some time going through Tensorflow tutorials and was curious what the learned kernels in the convolution layers looked like. After getting a visulization to work for regular RGB input images from CIFAR10, I transformed and created copies of the dataset using other representations: FFT, DCT, and HSV.  The same model used to train the RGB inputs was used and similar performance was achieved.&lt;/p&gt;

&lt;p&gt;If you would like to learn more you can implement it yourself &lt;a href=&#34;https://github.com/charlielehman/mbi&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Application of a Wavelet-Based Receiver for the Coherent Detection of FSK Signals</title>
      <link>https://charlielehman.github.io/publication/application-of-a-wavelet-based-receiver-for-the-coherent-detection-of-fsk-signals/</link>
      <pubDate>Sat, 01 Mar 2008 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/publication/application-of-a-wavelet-based-receiver-for-the-coherent-detection-of-fsk-signals/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
