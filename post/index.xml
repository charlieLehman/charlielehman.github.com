<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Charlie Lehman</title>
    <link>https://charlielehman.github.io/post/</link>
      <atom:link href="https://charlielehman.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 09 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://charlielehman.github.io/img/icon-192.png</url>
      <title>Posts</title>
      <link>https://charlielehman.github.io/post/</link>
    </image>
    
    <item>
      <title>Visualizing Softmax</title>
      <link>https://charlielehman.github.io/post/visualizing-tempscaling/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/visualizing-tempscaling/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I have been faced with several situations where visualizing the output of a trained classifier has helped explain some interesting behaviors that are present within learned representations.  I want to share a simple technique to visualize outputs for SoftMax-based classifiers.  As an example, I will walk through the process of visualizing and animating the effects of Temperature Scaling, which is a simple and useful technique for knowledge transfer [1] and model calibration [2][3][4].&lt;/p&gt;

&lt;h2 id=&#34;softmax&#34;&gt;Softmax&lt;/h2&gt;

&lt;p&gt;For deep neural networks (DNN) the representation is related to the construction of the optimization objective.
In the case of DNN image classifiers the most common objective is to minimize the softmax cross entropy between the model output, &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol{v}\in\mathbb{R}^k\)&lt;/span&gt; and a one-hot target, $\boldsymbol{y}$.
In order to compute the cross entropy, &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol{v}\)&lt;/span&gt; must first be projected onto a simplex to become &amp;quot;probability-like&amp;quot;.
&lt;span  class=&#34;math&#34;&gt;\(
\boldsymbol{\sigma}:\mathbb{R}^k\rightarrow\boldsymbol{\Delta}^{k-1}\\
\)&lt;/span&gt;
The resulting vector, $\boldsymbol{q}\in\boldsymbol{\Delta}^{k-1}$, is the output of the softmax operation, $\boldsymbol{\sigma}$.
To simplify notation, let &lt;span  class=&#34;math&#34;&gt;\(\mathbf{e}^{\boldsymbol{v}}= \left(\begin{smallmatrix}e^{v_0}&amp; e^{v_1}&amp;\dots&amp;e^{v_{k-1}}\end{smallmatrix}\right)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\boldsymbol{q} = \frac{\mathbf{e}^{\boldsymbol{v}}}{\sum^{k-1}_{i=0} e^{v_i}} 
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here&#39;s a visualization of SoftMax for the $k=2$ case.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./softmax.gif&#34; alt=&#34;gif&#34;&gt;&lt;/figure&gt;












  


&lt;video controls &gt;
  &lt;source src=&#34;softmax.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;

&lt;p&gt;It is then possible to assign a confidence, $c$, to a prediction by selecting the maximum component $c = \max(\boldsymbol{q})$.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./VisualizingTemperatureScaling_12_0.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;visualizing-softmax-at-higher-dimensions&#34;&gt;Visualizing Softmax at higher dimensions&lt;/h2&gt;

&lt;p&gt;My research mostly concerns classification or detection problems for images, which tends to involve more than 2 classes.  In order to visualize the behavior of softmax at $k&amp;gt;2$ we want to stay away from techniques that rely on the data directly (t-SNE, UMAP, PCA, etc.) and instead use a technique that can capture the macro behaviors in the representation space by prior construction.  Doing this is likely not without loss of information, because data-driven methods seek to find some transformation that preserves information.&lt;/p&gt;

&lt;p&gt;The first thing to do is to inspect the space to which softmax projects $\boldsymbol v$, the $(k-1)$-simplex  $\boldsymbol{\Delta}^{k-1}$, to better understand some useful properties for projection.  Loosely defined, a simplex is the generalization of the triangle.  In the case of the triangle it would be a 2-simplex.  Below I generated a tikz visualization of the 0 to 3 simplexes:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./VisualizingTemperatureScaling_15_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Take note of the graphical structure of simplexes as it will come into play in later discussion.  Now for a more formal definition of a simplex with regard to our softmax projection, &lt;span  class=&#34;math&#34;&gt;\(\boldsymbol{\sigma}:\mathbb{R}^k\rightarrow\boldsymbol{\Delta}^{k-1}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;As we previously defined, $\mathbf{q}$ is the resulting projected vector that follows the definition,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\{\mathbf{q}\in\mathbb{R}^k:\sum_i q_i=1, q_i\geq 0, i=0,\dots,k-1\},
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;which allows it to be used as a probability distribution.  The fact that all of the information is projected into the positive orthant is also useful as now every component, $q_i\in\mathbf{q}$, can be used to form a star-like graph about the origin:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./VisualizingTemperatureScaling_17_0.svg&#34; alt=&#34;svg&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;This projection is just a way to &lt;em&gt;view&lt;/em&gt; these components in such a way at arbitrary dimensions such that the angle between each when projected is equal.  And thus, forevermore throughout this post, such a technique shall be called the Equiradial Projection (EqR).&lt;/p&gt;

&lt;p&gt;Where the angle between components is&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\theta_i = \frac{2\pi (i+0.5)}{k}, \quad i=\{0,\dots,k-1\}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;and the projection matrix is&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\boldsymbol{T} = \begin{pmatrix}
\sin(\theta_0) &amp;\sin(\theta_1) &amp;\cdots &amp; \sin(\theta_{k-1}) \\ 
\cos(\theta_0) &amp;\cos(\theta_1) &amp;\cdots &amp; \cos(\theta_{k-1}) \\ 
\end{pmatrix},\\ 
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;making $\boldsymbol{T}\boldsymbol{q}$ just the weighted average of the projected components.  This is where distortions and loss of information come into play.  The derivation of the projection is based on the assumption that the space is star-like, meaning that each component occurs in isolation.  We know that this is in fact not the case most of the time as $\boldsymbol{q}$ by definition is a probability distribution making all but $k$ cases violate this assumption.&lt;/p&gt;

&lt;p&gt;Now that this loss of information and distortion is acknowledged we need to discuss rotation. When the order of the components changes in the projection the &lt;em&gt;view&lt;/em&gt; is just rotating about another component orthoganally.  This can provide some limited ability to minimize distortion by placing the most dependent subsets of components adjacent.&lt;/p&gt;

&lt;h2 id=&#34;temperature-scaling&#34;&gt;Temperature Scaling&lt;/h2&gt;

&lt;p&gt;Hinton et al. [1] introduced technique used for knowledge transfer called &amp;quot;Knowledge Distillation&amp;quot; that increases the &lt;em&gt;temperature&lt;/em&gt;, $T$, on $\boldsymbol{v}$ in order to generate a &lt;em&gt;softer&lt;/em&gt; representation out of softmax.  In [2], Guo et al,  refers to the same technique as &amp;quot;Temperature Scaling&amp;quot;:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\boldsymbol{q}_{temp} = \frac{\mathbf{e}^{\boldsymbol{v}/T}}{\sum^{k-1}_{i=0} e^{v_i/T}} , \qquad T\in\mathbb{R}^+
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This simple technique has proven to be incredibly useful beyond the initial proposed use in knowledge transfer. Guo et al. demonstrated its utility in calibrating deep models for image classification. In both [3][4], they use temperature scaling to great effect in detecting out-of-distribution samples.&lt;/p&gt;

&lt;p&gt;And without further ado, let&#39;s visualize temperature scaled softmax outputs in arbitrary dimensions!&lt;/p&gt;

&lt;p&gt;For $\boldsymbol{v}$, I just generated random vectors in the desired dimension and visualized the projection at a variation of $T$.  Each vector is colored according to the confidence at that location on the projection.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./tempscale_10.gif&#34; alt=&#34;gif&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;citations&#34;&gt;Citations&lt;/h2&gt;

&lt;p&gt;[1]: G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” ArXiv, vol. abs/1503.02531, 2015.&lt;/p&gt;

&lt;p&gt;[2]: C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in Proceedings of the 34th international conference on machine learning-volume 70, 2017, pp. 1321–1330.&lt;/p&gt;

&lt;p&gt;[3]: K. Lee, H. Lee, K. Lee, and J. Shin, “Training confidence-calibrated classifiers for detecting out-of-distribution samples,” in International conference on learning representations, 2018.&lt;/p&gt;

&lt;p&gt;[4]: S. Liang, Y. Li, and R. Srikant, “Enhancing the reliability of out-of-distribution image detection in neural networks,” in International conference on learning representations, 2018.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to segment CIFAR10</title>
      <link>https://charlielehman.github.io/post/weak-segmentation-cifar10/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/weak-segmentation-cifar10/</guid>
      <description>&lt;h2 id=&#34;import-important-things&#34;&gt;Import Important Things&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.models import resnet18
from torchvision.datasets import CIFAR10
from tqdm import tqdm_notebook as tqdm
from torchvision.utils import save_image, make_grid
from matplotlib import pyplot as plt
from matplotlib.colors import hsv_to_rgb
from matplotlib.image import BboxImage
from matplotlib.transforms import Bbox, TransformedBbox
import numpy as np
from IPython import display
import requests
from io import BytesIO
from PIL import Image
from PIL import Image, ImageSequence
from IPython.display import HTML
import warnings
from matplotlib import rc
import gc
import matplotlib
matplotlib.rcParams[&#39;pdf.fonttype&#39;] = 42
matplotlib.rcParams[&#39;ps.fonttype&#39;] = 42
gc.enable()
plt.ioff()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;initialize-the-tiny-model-from-resnet18&#34;&gt;Initialize the tiny model from ResNet18&lt;/h2&gt;

&lt;p&gt;I am replacing the first 7x7 conv stride of 4 with a 3x3 convolution kernel with stride of 1 and replacing maxpool with upsample.  This keeps the spatial features from being downsampled too quickly as the forward pass propagates.  The linear layer is replaced with a &amp;quot;pixelwise linear layer&amp;quot;, or a 1x1 convolution with stride of 1.  This can be simply thought of as projecting a 1x512 vector (pixel) with a 512x10 matrix (1x1 conv). Notice that there is no operation that performs a spatial aggregation so what we have left is a 10x32x32 tensor after the final upsample.  This can be used the same way as a semantic segmentation output, which we can also aggregate spatially and optimize using image level labels.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;num_classes = 10
resnet = resnet18(pretrained=True)
resnet.conv1 = nn.Conv2d(3,64,3,stride=1,padding=1)
resnet_ = list(resnet.children())[:-2]
resnet_[3] = nn.Upsample(scale_factor=2, mode=&#39;bilinear&#39;, align_corners=False)
classifier = nn.Conv2d(512,num_classes,1)
torch.nn.init.kaiming_normal_(classifier.weight)
resnet_.append(classifier)
resnet_.append(nn.Upsample(size=32, mode=&#39;bilinear&#39;, align_corners=False))
tiny_resnet = nn.Sequential(*resnet_)

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;define-attention&#34;&gt;Define Attention&lt;/h2&gt;

&lt;p&gt;In short, I&#39;m going to just define 0 as the threshold in the logit (pre-softmax space).  By selecting the largest component of the logit vector and then running it through sigmoid we can get a value with a support from 0 to 1, which is useful for inspecting the &amp;quot;attention&amp;quot; of the model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def attention(x):
    return torch.sigmoid(torch.logsumexp(x,1, keepdim=True))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cifar10-dataset&#34;&gt;CIFAR10 dataset&lt;/h2&gt;

&lt;p&gt;This dataset is so convenient for demonstrating so many things.  There are much more impressive demonstrations of weak segmentation, but all of this can be accomplished in a jupyter notebook so here we go!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=8),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

trainset = CIFAR10(root=&#39;.&#39;, train=True, download=True, transform=transform_train)
train_iter = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=16, pin_memory=True, drop_last=True)

testset = CIFAR10(root=&#39;.&#39;, train=False, download=True, transform=transform_test)
test_iter = DataLoader(testset, batch_size=100, shuffle=False, num_workers=16, pin_memory=True)

classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Files already downloaded and verified
Files already downloaded and verified
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;train-and-visualize&#34;&gt;Train and Visualize&lt;/h2&gt;

&lt;p&gt;The key take aways from the below code is that the objective for optimization is Binary Cross Entropy and the model&#39;s spatial aggregation is accomplished with a smooth-max operation.  This means after aggregation the vector is optimized to be a set of 10 binary detectors, which is in contrast to the most popular method of characterization: softmax cross entropy, which encourages each pixel to select only one.  When combined with the aforementioned attention operation we can forego aggregation and directly inspect exactly what the model uses to make a decision!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = nn.DataParallel(tiny_resnet).cuda()
num_epochs = 10
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9, weight_decay=1e-4)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,78,eta_min=0.001)

losses = []
acces = []
v_losses = []
v_acces = []
for epoch in tqdm(range(num_epochs)):
    epoch_loss = 0.0
    acc = 0.0
    var = 0.0
    model.train()
    train_pbar = train_iter
    for i, (x, _label) in enumerate(train_pbar):
        x = x.cuda()
        _label = _label.cuda()
        label = F.one_hot(_label).float()
        seg_out = model(x)
        
        attn = attention(seg_out)
        # Smooth Max Aggregation
        logit = torch.log(torch.exp(seg_out*0.5).mean((-2,-1)))*2
        loss = criterion(logit, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        epoch_loss += loss.item()
        acc += (logit.argmax(-1)==_label).sum()
        #train_pbar.set_description(&#39;Accuracy: {:.3f}%&#39;.format(100*(logit.argmax(-1)==_label).float().mean()))
        
    avg_loss = epoch_loss / (i + 1)
    losses.append(avg_loss)
    avg_acc = acc.cpu().detach().numpy() / (len(trainset))
    acces.append(avg_acc)
    model.eval()
    epoch_loss = 0.0
    acc = 0.0
    num_seen = 0
    
    test_pbar = tqdm(test_iter)
    for i, (x, _label) in enumerate(test_pbar):
        x = x.cuda()
        _label = _label.cuda()
        label = F.one_hot(_label).float()
        seg_out = model(x)
        attn = attention(seg_out)
        logit = torch.log(torch.exp(seg_out*0.5).mean((-2,-1)))*2
        loss = criterion(logit, label)
        epoch_loss += loss.item()
        acc += (logit.argmax(-1)==_label).sum()
        num_seen += label.size(0)
        test_pbar.set_description(&#39;Accuracy: {:.3f}%&#39;.format(100*(acc.float()/num_seen)))
    
    avg_loss_val = epoch_loss / (i + 1)
    v_losses.append(avg_loss_val)
    avg_acc_val = acc.cpu().detach().numpy() / (len(testset))
    v_acces.append(avg_acc_val)
    plt.close(&#39;all&#39;)

    conf = torch.max(nn.functional.softmax(seg_out, dim=1), dim=1)[0]
    hue = (torch.argmax(seg_out, dim=1).float() + 0.5)/10
    x -= x.min()
    x /= x.max()
    gs_im = x.mean(1)
    gs_mean = gs_im.mean()
    gs_min = gs_im.min()
    gs_max = torch.max((gs_im-gs_min))
    gs_im = (gs_im - gs_min)/gs_max
    hsv_im = torch.stack((hue.float(), attn.squeeze().float(), gs_im.float()), -1)
    im = hsv_to_rgb(hsv_im.cpu().detach().numpy())
    ex = make_grid(torch.tensor(im).permute(0,3,1,2), normalize=True, nrow=25)
    attns = make_grid(attn, normalize=False, nrow=25)
    attns = attns.cpu().detach()
    inputs = make_grid(x, normalize=True, nrow=25).cpu().detach()
    display.clear_output(wait=True)
    plt.figure(figsize=(20,8))
    plt.imshow(np.concatenate((inputs.numpy().transpose(1,2,0),ex.numpy().transpose(1,2,0), attns.numpy().transpose(1,2,0)), axis=0))
    #plt.xticks(np.linspace(18,324,10), classes)
    #plt.xticks(fontsize=20) 
    plt.yticks([])
    plt.title(&#39;CIFAR10 Epoch:{:02d}, Train:{:.3f}, Test:{:.3f}&#39;.format(epoch, avg_acc, avg_acc_val), fontsize=20)
    display.display(plt.gcf())
    fig, ax = plt.subplots(1,2, figsize=(20,8))
    ax[0].set_title(&#39;Crossentropy&#39;)
    ax[0].plot(losses, label=&#39;Train&#39;)
    ax[0].plot(v_losses, label=&#39;CIFAR10 Test&#39;)
    ax[0].legend()
    ax[1].set_title(&#39;Accuracy&#39;)
    ax[1].plot(acces, label=&#39;Train&#39;)
    ax[1].plot(v_acces, label=&#39;CIFAR10 Test&#39;)
    ax[1].legend()
    display.display(plt.gcf())
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I only trained it for 10 epochs here and get a passable performance, which does improve if it goes further.  I stopped it to leave some of the mixed decisions the model is making.  Declaring success here is premature for calling this as a great method for weak segmentation, but it does show exactly what the model considers spatially for every decision it makes.  Once more, the model only uses values that are very positive thus saturating sigmoid to make a decision, by combining the argmax with the attention operation defined we can get the below visualization.  The examples that show multiple colors are examples that share features with other classes, i.e. the birds and airplanes, deer and horses, cars and trucks.  Now if there were only a method for looking at where all the pixels project into the learned space all at once!&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./Weak_Segmentation_of_CIFAR10_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Though not absolutely necessary here&#39;s what the numbers look like during training.
&lt;figure&gt;&lt;img src=&#34;./Weak_Segmentation_of_CIFAR10_7_1.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speechworks Interview</title>
      <link>https://charlielehman.github.io/post/speechworks-interview/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/speechworks-interview/</guid>
      <description>&lt;p&gt;I was interviewed by Julie Lindsay from Speechworks for a &lt;a href=&#34;https://www.speechworks.net/podcast/podcast-the-entrepreneurs-secret-why-a-perfect-pitch-isnt-just-about-ideas/&#34;&gt;podcast&lt;/a&gt; regarding my experiences starting ConvexMind during the CreateX Startup Launch last Summer. There is also an &lt;a href=&#34;https://www.speechworks.net/the-entrepreneurs-secret-why-a-perfect-pitch-isnt-just-about-ideas/&#34;&gt;article&lt;/a&gt; that summarizes the main points we discussed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Adversarial Attacks</title>
      <link>https://charlielehman.github.io/post/visualizing-fgsm/</link>
      <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/visualizing-fgsm/</guid>
      <description>&lt;h2 id=&#34;pixelwise-classifiers-with-largemargin-gaussian-mixture-representation&#34;&gt;Pixelwise classifiers with Large-margin Gaussian Mixture representation&lt;/h2&gt;

&lt;p&gt;These are several visualizations I dug up from really really messy code, which I won&#39;t share just yet. This was some quick and dirty work to test out the feasibility of an idea: I wanted to visualize the effects on the representation space at each pixel during an &lt;a href=&#34;https://arxiv.org/abs/1412.6572&#34;&gt;FGSM&lt;/a&gt; attack.  I trained a &lt;a href=&#34;https://charlielehman.github.io/post/weak-segmentation-cifar10&#34;&gt;weak segmentor model&lt;/a&gt; with the representation space modified for &lt;a href=&#34;https://arxiv.org/abs/1803.02988&#34;&gt;L-GM&lt;/a&gt;.  I found L-GM useful because it provides a sensible way of directly learning a representation that can be visualized without further solving an additional optimization problem (looking at you &lt;a href=&#34;www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&#34;&gt;T-SNE&lt;/a&gt;!).  Not to mention you can project new points into it, which was really central to my purpose of observing the effects of an adversarial attack.
&lt;figure&gt;&lt;img src=&#34;orig.png&#34; alt=&#34;Original&#34; title=&#34;Original Truck Image&#34;&gt;&lt;figcaption&gt;Original Truck Image&lt;/figcaption&gt;&lt;/figure&gt;
This is what a normal input from the CIFAR10 test set looks like.&lt;/p&gt;

&lt;p&gt;The max-softmax, which is a &lt;a href=&#34;https://github.com/hendrycks/error-detection&#34;&gt;good baseline&lt;/a&gt; for confidence in detection tasks drops from 1.00 to 0.72 after adversarial modification. I also computed a confidence term that looked for regions of &amp;quot;uncertainty&amp;quot;, which I am using loosely.  Specifically I was interested in regions close to 0.5 when using sigmoid on the raw logits and the logits post &lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34;&gt;temperature scaling&lt;/a&gt;. I saw that when the model is behaving well there is a uniform expansion of the uncertain regions spatially resulting in little overlap between the raw and temperature scaled regions.  So by multiplying both taking the spatial average I can determine the overall uncertainy and by additive inverse get a confidence measure for observing adversarial images spatially.
&lt;figure&gt;&lt;img src=&#34;fgsm.png&#34; alt=&#34;FGSM&#34; title=&#34;Original Truck Image&#34;&gt;&lt;figcaption&gt;Original Truck Image&lt;/figcaption&gt;&lt;/figure&gt;
This is what an untargeted FGSM attack looks like.&lt;/p&gt;

&lt;p&gt;In this case it appears that FGSM just messed with the image&#39;s high contrast regions on the cab of the truck.  My confidence measure drops to 0.64 from 0.92, but the detection still clears the 0 threshold sigmoid learned.&lt;/p&gt;

&lt;p&gt;When &lt;a href=&#34;https://ieeexplore.ieee.org/document/6968381&#34;&gt;CLAHE&lt;/a&gt; is applied to both images we can see some interesting effects.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;orig-eqhist.png&#34; alt=&#34;Original with CLAHE&#34; title=&#34;Original Truck Image&#34;&gt;&lt;figcaption&gt;Original Truck Image&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;On the original image, my confidence measure drops from 0.92 to 0.84 and the pixel embedding spreads out far more.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;fgsm-eqhist.png&#34; alt=&#34;FGSM with CLAHE&#34; title=&#34;Original Truck Image&#34;&gt;&lt;figcaption&gt;Original Truck Image&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;On the FGSM image, my confidence measure plummets from 0.64 to 0.21.  Interestingly, the pixel embedding stretches back toward truck.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Basis Input Convolutional Neural Network</title>
      <link>https://charlielehman.github.io/post/multi-basis-input/</link>
      <pubDate>Tue, 31 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://charlielehman.github.io/post/multi-basis-input/</guid>
      <description>&lt;p&gt;I spent some time going through Tensorflow tutorials and was curious what the learned kernels in the convolution layers looked like. After getting a visulization to work for regular RGB input images from CIFAR10, I transformed and created copies of the dataset using other representations: FFT, DCT, and HSV.  The same model used to train the RGB inputs was used and similar performance was achieved.&lt;/p&gt;

&lt;p&gt;If you would like to learn more you can implement it yourself &lt;a href=&#34;https://github.com/charlielehman/mbi&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
